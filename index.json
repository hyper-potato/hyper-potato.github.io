[{"categories":["website"],"content":"I’ve tried Jekyll, Hexo and Hugo and now my website uses Hugo. It is simple, boring, flexible, and fast. The main reason is that it is simple. There’s not much you have to learn to get started. Let’s cut to the chase. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:0:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"Assumptions You know how to open and run simple lines in the macOS or Windows terminal. You will use ~/Sites as the starting point for your site. (~/Sites is used for example purposes. If you are familiar enough with the command line and file system, you should have no issues following along with the instructions.) You can write content in Markdown. You have git installed and linked to your github account. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:1:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"1. Install Hugo Mac using home-brew brew install hugo Note to install extended version of Hugo. More operation systems and install methods can be found on install instructions of Hugo. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:2:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"2. Create a New Site Open terminal and navigate to the folder that you want to work on your blog and then: # replace \u003chyper-potato.github.io\u003e with \u003cyour-github-username.github.io\u003e hugo new site hyper-potato.github.io This folder is the root directory for all activities after. The best practice of naming is \u003cyour-github-username.github.io\u003e if you are going to use Github Pages to host the site. The directory structure should be like this . ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── static └── themes Articles in markdown will be placed under content folder. Photos and favicon can be saved in static. More details in each component is explained here. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:3:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"3. Pick a theme Note before you clone and use the theme you choose, make sure the theme supports math and code to avoid hassle later. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:4:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"1) Get the theme Here is an awesome Hugo theme compilation. Git clone your choice of theme. Here I use vitae as an example. You can install the theme by git clone. git clone https://github.com/datacobra/hugo-vitae.git themes/hugo-vitae ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:4:1","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"2) Quick config You can also copy example content and default config file for a quick start # Copy the default site config and example content cp -r themes/hugo-vitae/exampleSite/* ./ More details see here To configure your site according to your needs, just open the config.toml and adjust the settings. All options you should customize to yours. Make sure to read all the comments, as there a few nuances with Hugo themes that require some changes to that file. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:4:2","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"3) Creating a post hugo new posts/hotday.md # new post You can now go ahead an edit the newly created file under the content directory. Once you are finished editing, to have hugo generate the page, set draft = false in the articles front matter. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:4:3","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"4) Organizing pages The above example demonstrates how to create a pages and posts. Hugo automatically applies the list templates for a directory of posts, which works well for blogs and posts. However, you may want at times want to override this behavior and create a standalone page (like an about page or projects page) or have more control of what content is listed from within the directory. In such cases, you can override the default behavior by placing an index.md file in the corresponding content directory. hugo new projects/index.md ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:4:4","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"5) Test and configure your site Start hugo server locally, hugo server and visit http://localhost:1313/ to see your site. All options you can and should customize are commented. By default hugo will watch your files for any changes you make and automatically rebuild the site. It will then live reload any open browser pages and push the latest content to them. Note: If you are seeing a blank page it is probably because you have nothing in your content/ directory. Read on to fix that. Again, check out documentation and play around with features along the way. After the test is done, kill the server by ctrl + C ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:4:5","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"4. Hosting a Hugo website A Hugo blog is completely static. GitHub Pages is a great place where you can host a Hugo blog, for free. But .github.io site also looks okayish. Here I deploy Hugo as a GitHub personal site as an example. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:5:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"1) Create a Github repo Create a new repo with the same name as the project folder (your-github-username.github.io) on Github website. This repo is used to store all the content in the project folder. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:5:1","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"2) Initialize the blog folder git init git add . git commit -m \"Initial commit\" After the repo is initialized, run the following command in the blog folder to push blog files. git remote add origin https://github.com/your-github-username/your-github-username.github.io.git git push -u origin master For detailed instructions on how to host website on Github, go to here. ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:5:2","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"3) Publish the public folder Here is a bash file to deploy site, less hassle. Save the following as deploy.sh and then run the script each time from blog root directory to deploy the site. #!/bin/bash # check if public folder exists hugoPath=`pwd` publicPath=`pwd`\"/public\" if [ ! -d $publicPath ];then echo public not exist exit fi # remove all files generated before in public/ cd $publicPath rm -r ./* # Back to parent folder and re-generate cd $hugoPath hugo # This is the line to build the website! # go to public and publish cd $publicPath git add -A . time=$(date \"+%Y-%m-%d %H:%M:%S\") commit=\"Site updated:\"$time echo $commit git commit -m \"$commit\" git push origin master sh deploy.sh ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:5:3","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"4) Back up source code The hugo command will generate static files of the website in the public folder. However, Github will only load code and publish site from master branch or master branch /docs by default. One solution to this is to push only public directory to master branch, but create a hugo branch in the same repo to place source code. cd .. # back to root directory git checkout -b hugo git add . git commit -m \"commit source code\" git push -u origin hugo ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:5:4","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["website"],"content":"Have Fun! After playing around with css and html for long, I’m kinda into website development now… ","date":"2020-06-19","objectID":"/2020-06-29-blog-with-github-hugo/:6:0","tags":["blog","Hugo"],"title":"Build your blog with Github + Hugo Part 1","uri":"/2020-06-29-blog-with-github-hugo/"},{"categories":["machine learning"],"content":"In this part one blog about categorical encoding, I will be introducing some common encoding methods that I have explored and pros and cons for each of them. ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:0:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"TL; DR There is no ‘one size fits all’, and it usually depends on your data representations and requirements. If a variable has a lot of categories then a one-hot encoding scheme will produce many columns which can cause memory issues. In my experience relying on LightGBM/CatBoost is the best out-of-the-box method. Label encoding is a road to nowhere in most scenarios and you should be careful using it. However if your categorical variable happens to be ordinal then you can and should represent it with increasing numbers (for example “cold” becomes 0, “mild” becomes 1, and “hot” becomes 2). word2vec and others such methods are cool and good but they require some fine-tuning and don’t always work out. No matter what method do you choose, always consider how to deal with new categories in test data that not appear in train data. ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:1:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"One-hot encoding Assume we all have prior knowledge about creating dummy variables for categorical data. I am being judgemental and personally not a fan for OHE for two reasons: One-hot encoding might not be the best way to preprocess the data, especially for tree-based learners. For high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deeeeep to achieve good accuracy. There will be mismatch in train and test set. Specifically, after one-hot encoding the number of columns in the training and test set data can be unequal, and therefore, the model will throw error during predicting. Workaround: Get the missing columns and add them to the test dataset: (thanks to answer from stackoverflow # Get missing columns in the training test missing_cols = set( train.columns ) - set( test.columns ) # Add a missing column in test set with default value equal to 0 for c in missing_cols: test[c] = 0 # Ensure the order of column in the test set is in the same order than in train set train, test = train.align(test, axis=1) This code also ensure that column resulting from category in the test dataset but not present in the training dataset will be removed. ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:2:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"Label Encoding One common way to deal with categories is to simply map each category with a number. By applying such transformation, a model would treat categories as ordered integers, which in some cases is wrong. Such transformation should not be used “as is” for several types of models (Linear Models, KNN, Neural Nets, etc.). # Code for encoding multiple categorical columns from sklearn.preprocessing import LabelEncoder le=LabelEncoder() # Iterating over all the common columns in train and test for col in X_test.columns.values: # Encoding only categorical variables if X_test[col].dtypes=='object': # Using whole data to form an exhaustive list of levels data= X_train[col].append(X_test[col]) le.fit(data.values) X_train[col]=le.transform(X_train[col]) X_test[col]=le.transform(X_test[col]) ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:3:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"Optimal Binning in tree-learners LightGBM or CatBoost have built-in Optimal binning which is very convinent and well-performed. In fact in my experience relying on LightGBM/CatBoost is the best out-of-the-box method. While applying gradient boosting it could be used only if the type of a column is specified as “category”. New categories in Label Encoder are replaced with “-1” or None. If you are working with gradient boosting model (especially Lightgbm) , LE is the simplest way to work with categories in terms of memory (the category type in python consumes much less memory than the object type). category_col = data.select_dtypes('object').columns.tolist() for c in category_col: X_train[c] = X_train[c].astype('category') X_test[c] = X_test[c].astype('category') d_train = lgb.Dataset(X_train, label=y_train, free_raw_data=False, feature_name=X.columns.tolist(), categorical_feature=cat_col) # specify categorical columns while forming train and valid data matrix Upon further reflection, for LightGBM, it looks like simply using the built-in categorical encoding is outperforming any kind of categorical encoding I can personally do. ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:4:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"Target Encoder (TE) I have encoutered many TE on kaggle winners solutions so I decided to try it out. It takes information about the target to encode categories, which makes it extremely powerful. The encoded category values are calculated according to the following formulas: Target encoding is a fast way to get the most out of your categorical variables with little effort. The idea is quite simple. Say we can have a categorical variable $X$ and a target $Y$. For each distinct element in $X$ we’re going to compute the average of the corresponding values in $Y$ (It doesn’t matter whether Y is continuous or binary). Then we’re going to replace each $X_i$ with the according mean. This is rather easy to do in Python and the pandas library. First let’s create some dummy data. import pandas as pd df = pd.DataFrame({ 'x_0': ['a'] * 3 + ['b'] * 3, 'x_1': ['c'] * 1 + ['d'] * 5, 'y': [1, 0, 1, 0, 1, 0]}) Here’s how data looks like. x_0 x_1 y 0 a c 1 1 a d 1 2 a d 1 3 b d 0 4 b d 1 5 b d 0 Replace each value in x_0 with the matching mean. df['x_0'] = df['x_0'].map(df.groupby('x_0')['y'].mean()) df['x_1'] = df['x_1'].map(df.groupby('x_1')['y'].mean()) We now have the following data frame. x_0 x_1 y 0 0.666667 1.0 1 1 0.666667 0.4 0 2 0.666667 0.4 1 3 0.333333 0.4 0 4 0.333333 0.4 1 5 0.333333 0.4 0 Target encoding is good because it picks up values that can explain the target. In this vanilla example value a of variable x_0 has an average target value of 0.67. This can greatly help the machine learning classifications algorithms used downstream. The problem of target encoding is target leakage. It uses information about the target. Because of the target leakage, model overfits the training data. In the example, the value d of variable x_1 is replaced with a 1 because it only appears once and the corresponding value of y is a 1. In this case we’re over-fitting because we don’t have enough values to be sure that 1 is in fact the mean value of y when x_1 is equal to d. In other words only relying on each group mean is too reckless. A popular way to reduce target leakage is to use cross-validation and compute the means in each out-of-fold dataset. This is what H20 does and what many Kagglers do too. Another approach is to use additive smoothing. This is supposedly what IMDB uses to rate it’s movies. ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:5:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"James-Stein Encoder James-Stein Encoder is a target-based encoder. The idea behind James-Stein Encoder is simple. Encoding is aimed to improve the estimation of the category’s mean target (first member of the amount) by shrinking them towards a more central average (second member of the amount). See more in the document. ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:6:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["machine learning"],"content":"Helmert Encoder Helmert coding compares each level of a categorical variable to the mean of the subsequent levels. Hence, the first contrast compares the mean of the dependent variable for “A” with the mean of all of the subsequent levels of categorical column (“B”, “C”, “D”), the second contrast compares the mean of the dependent variable for “B” with the mean of all of the subsequent levels (“C”, “D”), and the third contrast compares the mean of the dependent variable for “C” with the mean of all of the subsequent levels (in our case only one level — “D”). This type of encoding can be useful in certain situations where levels of the categorical variable are ordered, say, from lowest to highest, or from smallest to largest. Reference https://www.kaggle.com/c/avito-demand-prediction/discussion/55521 https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features https://stackoverflow.com/questions/21057621/sklearn-labelencoder-with-never-seen-before-values ","date":"2020-06-04","objectID":"/2020-06-04-deal-with-categorical-variable/:7:0","tags":["machine learning","feature engineering"],"title":"Categorical variables encoding in machine learning -Part1","uri":"/2020-06-04-deal-with-categorical-variable/"},{"categories":["shower thought"],"content":".container { max-width: 1200px; margin-left: auto; margin-right: auto; } I think yesterday was a Friday because someone used #flashbackfriday but every day brings painful flashbacks. I miss the hustle and bustle of a crowded crossroad. I miss sitting on a seat and it’s still warm and then kind of feels wet and you’re not entirely sure but you’ve already committed to that seat and then finally when you get up you think yeah it was wet. The world never was my oyster and now I can barely make my playground further than 20 yards from me. But I miss roaming around going through airport security and having the agents say ‘have a good flight’, and I say, ‘you too’. Did I recognize that last time I would be introduced to a stranger and immediately forget their name within 30 seconds? I think she said she worked for a cafe. Was she Nancy, maybe Nicole. I’m pretty sure it started with an N but to be honest I really wasn’t listening. How naive I was on those dates in a bar with a live band and I’m trying to tell a story but then the song ends and it gets really quiet and I’m still yelling in his ear. And those moments when I recognized someone and made eye contact with but then played it off like I didn’t see them. Would you know what I was thinking? Like your body is your mind in isolation. I’ll never know. 🌈 ","date":"2020-05-11","objectID":"/2020-05-11-isolation/:0:0","tags":null,"title":"Day 60 but who's keeping count","uri":"/2020-05-11-isolation/"},{"categories":["EDA"],"content":"Sit tight and listen to music For the past weeks, I’ve taken breaks from staring blankly into the middle distance to dip deeper into my playlists than I have in years. I’ve been using Spotify only since last year since most of time I used apple music. I like the feature of adding songs to library on apple music. By innertion I’m still using the library to save songs that I like. After moving apple music library to Spotify, I nerded out to see how my playlist looks under Spotify’s standard. All code can be found here. ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:0:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Preparation To get the data I used Spotify API and spotipy as a Python client. After creating a web application in Spotify API Dashboard and gathering the credentials, I was able to initialize and authorize the client. import spotipy import spotipy.util as util user_id = 'your user_id' client_id= 'your client_id' client_secret= 'your client_secret' token = util.prompt_for_user_token(user_id, scope = 'user-top-read playlist-read-collaborative', client_id=client_id, client_secret=client_secret, redirect_uri= redirect_uri) # arbitrary url you put in while registering in Spotify API sp = spotipy.Spotify(auth=token) ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:1:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Recent top songs So here I’m gonna pull up my most played tracks in the last 4 weeks. if token: sp = spotipy.Spotify(auth=token) artist_shortterm = [] song_shortterm = [] results = sp.current_user_top_tracks(time_range='short_term', limit=50) \"\"\" time_range: long_term (calculated from several years of data), medium_term (approximately last 6 months), short_term (approximately last 4 weeks) \"\"\" for i, item in enumerate(results['items']): song_shortterm.append(item['name']) artist_shortterm.append(item['artists'][0]['name']) pd_top50 = pd.DataFrame({'track':song_shortterm, 'artist':artist_shortterm }) pd_top50.sample(12) track artist What I Got Sublime Lovesong The Cure Heart-Shaped Box Nirvana Just Like Honey The Jesus and Mary Chain Gigantic (live) Pixies The Diamond Sea Sonic Youth Hey Pixies Schizophrenia Sonic Youth Feel Good Inc. Gorillaz Psycho Killer Talking Heads Lamb Of God Marilyn Manson Smelly Cat Medley Phoebe Buffay And The Hairballs ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:2:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Get audio features of song tracks As everything is inside just one playlist, it was easy to gather. The only problem was that user_playlist method in Spotipy doesn’t support pagination and can only return the first 100 track, but it was easily solved by adding condition of while more_songs def get_features_from_playlist(user='', playlist_id=''): df_result = pd.DataFrame() track_list = '' added_ts_list = [] artist_list = [] title_list = [] more_songs = True #As long as there is tracks not fetched from API, continue looping offset_index = 0 if playlist_id != '' and user == '': print(\"Enter username for playlist\") return while more_songs: songs = sp.user_playlist_tracks(user, playlist_id=playlist_id, offset=offset_index) for song in songs['items']: track_list += song['track']['id'] +',' added_ts_list.append(song['added_at']) title_list.append(song['track']['name']) artists = song['track']['artists'] artists_name = '' for artist in artists: artists_name += artist['name'] + ',' artist_list.append(artists_name[:-1]) track_features = sp.audio_features(track_list[:-1]) df_temp = pd.DataFrame(track_features) df_result = df_result.append(df_temp) track_list = '' if songs['next'] == None: more_songs = False else: offset_index += songs['limit'] print('Progress: ' + str(offset_index) + ' of '+ str(songs['total'])) #add the timestamp added, title and artists of a song df_result['added_at'], df_result['song_title'], df_result['artists'] = added_ts_list, title_list, artist_list return df_result ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:3:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"A glimpse of my playlists get all my playlist: user_playlists = sp.user_playlists(user='lalala') for playlist in user_playlists['items']: print(playlist['id'], playlist['name']) id1 TUNE xx2 Sonic Youth Radio xx3 Driving xx4 Kickkkk ... First column is playlist id, second is the name of my playlists. Let’s dive into my quarantine playlist ‘TUNE’ 🙌 playlist = sp.user_playlist(user_id, 'what ever id1 is') tracks = playlist['tracks']['items'] next_uri = playlist['tracks']['next'] for _ in range(int(playlist['tracks']['total'] / playlist['tracks']['limit'])): response = sp._get(next_uri) tracks += response['items'] next_uri = response['next'] tracks_df = pd.DataFrame([(track['track']['id'], track['track']['artists'][0]['name'], track['track']['name'], parse_date(track['track']['album']['release_date']) if track['track']['album']['release_date'] else None, parse_date(track['added_at'])) for track in playlist['tracks']['items']], columns=['id', 'artist', 'name', 'release_date', 'added_at'] ) ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:4:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Top artists The first vanilla idea was the list of the most appearing artists in my playlist: tracks_df.groupby('artist').count()['id'].reset_index().sort_values('id', ascending=False).rename(columns={'id': 'amount'}).head(10) Artist amount Sublime 20 Dire Straits 18 The Cure 13 BANKS 12 Radiohead 11 Pink Floyd 11 Oasis 11 Eminem 11 Nirvana 11 Gorillaz 11 ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:4:1","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Audio features of song tracks Spotify API has an endpoint that provides features like danceability, energy, loudness and etc for tracks. So I gathered features for all tracks from the playlist. I don’t have years of records on Spotify so it’s difficult to check how my taste has changed over years. 🤷‍♀️ Getting sad? So I looked at if my music habits changes under lockdown. It turns out only Valence had some visible difference: Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). Ahh. Lockdown unleashed my sadness just like the old saying from me ‘Depression feels like my 30-pound dog is sitting on my chest’. Kidding. I luuuuuv my dog!! plt.figure(figsize=(6,4)) sns.boxplot(x=df_2020.added_at.dt.month, y=df_2020.valence, color='#1eb954') plt.title(\"Valence changes over months\", fontsize=12,y=1.01,weight='bold') plt.show() Have moves? Huh? I have no words. This is a shame to someone who has 100% danceability!!! To get my head around the loss of my danceability, let’s check if my sadness has anything to do with the dancing! tracks_w_features.plot(kind='scatter', x='danceability', y='valence') plt.title(\"Danceability x Valence\", fontsize=12, y=1.01,weight='bold') plt.tight_layout() Hmm. Interesting. So apperantly my playlist doesn’t show the positive correaltion between ‘upbeating’ and ‘danceable’. :thinking: Let’s just say I’ve tried to be chipper under quarantine, because I’m afraid that if there’s one crack, I’ll fall apart completely. ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:4:2","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"How different and similar among songs? I took those features out and calculate the distance between every two different tracks. (matrix production) encode_fields = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'] def encode(row): return np.array([ (row[k] - tracks_with_features_df[k].min()) / (tracks_with_features_df[k].max() - tracks_with_features_df[k].min()) for k in encode_fields]) tracks_with_features_encoded_df = tracks_with_features_df.assign( encoded=tracks_with_features_df.apply(encode, axis=1)) tracks_w_features_encoded_product = tracks_w_features_encoded.assign(temp=0) \\ .merge(tracks_w_features_encoded.assign(temp=0), on='temp', how='left').drop(columns='temp') tracks_w_features_encoded_product = tracks_w_features_encoded_product[ tracks_w_features_encoded_product.id_x != tracks_w_features_encoded_product.id_y] tracks_w_features_encoded_product['merge_id'] = tracks_w_features_encoded_product \\ .apply(lambda row: ''.join(sorted([row['id_x'], row['id_y']])), axis=1) tracks_w_features_encoded_product['distance'] = tracks_w_features_encoded_product \\ .apply(lambda row: np.linalg.norm(row['encoded_x'] - row['encoded_y']), axis=1) ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:5:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Most similar songs After that I was able to get most similar songs/songs with the minimal distance, and it selected kind of similar songs: tracks_w_features_encoded_product.sort_values('distance').drop_duplicates('merge_id') \\ [['artists_x', 'song_title_x', 'artists_y', 'song_title_y', 'distance']].head(10) artists_x song_title_x artists_y song_title_y distance Florence + The Machine The End Of Love Glass Animals Gooey 0.011732 The Stone Roses Love Spreads The Stone Roses Love Spreads 0.038285 OK Go Here It Goes Again The Jesus and Mary Chain Some Candy Talking 0.108457 The Libertines Can’t Stand Me Now Foo Fighters Monkey Wrench 0.117521 AC/DC Thunderstruck Muse Starlight 0.141387 Marilyn Manson The Nobodies Foo Fighters My Hero 0.147820 Pulp Something Changed Ween Mutilated Lips 0.158513 Blur My Terracotta Heart Men I Trust Tailwhip 0.161328 Talking Heads Road to Nowhere HAIM The Steps 0.162264 Halestorm Bad Romance Dodgy Good Enough 0.162886 Suprisely makes sense! ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:5:1","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Most average songs i.e. the songs with the least distance from every other song: tracks_w_features_encoded_product \\ .groupby(['artists_x', 'song_title_x']) \\ .sum()['distance'] \\ .reset_index() \\ .sort_values('distance') \\ .head(10) artists song_title distance The Animals We Gotta Get Out Of This Place 758.802868 Tenacious D Fuck Her Gently 761.310917 Arctic Monkeys Do I Wanna Know? 767.353926 One Direction Story of My Life 773.588932 Urge Overkill Girl, You’ll Be a Woman Soon 775.938550 alt-J Tessellate 783.974366 Guns N’ Roses Knockin’ On Heaven’s Door 786.414124 alt-J Breezeblocks 787.258564 Divinyls I Touch Myself 787.683498 Tears For Fears,Dave Bascombe Head Over Heels - Dave Bascombe 7” N.Mix 789.882583 ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:5:2","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Most ‘So not me’ songs artists_x song_title_x distance The Stone Roses Love Spreads 1859.548582 Piano Dreamers Heaven’s Gate 1504.676314 Per-Olov Kindgren After Silence 1406.159590 Men I Trust All Night 1348.317181 MONO Ashes in the Snow - Remastered 1324.674595 ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:5:3","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Next I’ll use cluster to bucket my favorite tracks when I get really bored. ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:6:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":["EDA"],"content":"Since you made this far How do you find good music you’ve never heard before? Try this gem I recently found… www.gnoosic.comyou put in three of your favorite bands/artists, and it will recommend similar stuff that you most likely haven’t listened to. And then you will come back and thank me. You are welcome! Stay safe. ","date":"2020-04-05","objectID":"/2020-04-05-when-we-are-under-quarantine/:7:0","tags":["EDA","Python","music","data analysis"],"title":"A boring look into my Spotify playlist","uri":"/2020-04-05-when-we-are-under-quarantine/"},{"categories":null,"content":"Nina. This person is currently… 🚶‍♀️: Roaming around 👩‍🍳: Burning the kitchen 📺 : Babylon Berlin I’m a graduate student from University of Minnesota (Class 2020). This blog is to document my 🎒analytics study journey, and mostly, shower thoughts. Read with caution. May the blessing of Bayes be upon you. ","date":"2020-04-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"So… how was this site built? With blood and tears. 😬 This is a Jekyll Hugo site, hosted on GitHub Pages. I am working on migrating it to Netlify though. It was built after months of procrastination and hours upon hours of pressing ⌘-⇧-R, resizing windows ploughing through documentation, googling and of course, some actual coding. ","date":"2020-04-02","objectID":"/about/:0:1","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"You can find me almost ERRYWURRR 🖖 💻github 🐦twitter 👬facebook 💼linkedin 📧email ","date":"2020-04-02","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Visualization"],"content":"Self-quarantine gets me nerd-out on small things that I usually googled again and again. For example, very often I need to plot multiple plots sharing the same category, where it’s better to keep colors across categories consistent for all the plots. The easiest solution to make sure to have the same colors for the same categories in different plots would be to manually specify the colors at plot creation. Alternatively, we can use a list of colors or a dictionary to map colors onto categories. I will be using dictionary in the following example. The example is to plot New York City housing price and rental price across boroughs. Import packages import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import pyplot_themes as themes %matplotlib inline %config InlineBackend.figure_format = 'retina' Don’t repeat ourselves, so… If you have particular settings that you want to apply to all of your plots, you can use Matplotlib rcParams to do so. Changing the settings of rcParams will affect all subsequent plots. For example, if you want all plots to have the size 12 x 8 inches instead of the default 6 x 4 inches, you can write: plt.rc('figure', figsize=(12, 8)) To change rcParams settings, you pass first the group you want to change its settings (in the example above, the group is 'figure') then you pass the settings you want to change with their values. For a complete list of rcParams, click here. specify palette with customized colors self_palette ={'Brooklyn':'#56bdff', 'Manhattan':'#07335d', 'Staten Island':'#a9aaab', 'Queens':'#ff9856', 'Bronx':'#ecd1cb'} Mapping values from the hue column to colors in the plot, assign palette in palette = self_palette # Median housing price within boroughs from 2010 to 2020 plt.figure(figsize=(12,6)) themes.theme_ucberkeley() data = z_nyc[z_nyc['zipcode'].isin(newdata_2020.zipcode.unique())] sns.lineplot(data = data, x='yr_mth', y='value',hue='borough', palette=self_palette, ) plt.xlabel('Year',fontsize=15,labelpad=8) plt.ylabel('House Price',fontsize=15,labelpad=8) plt.title('Housing price by borough from 2010 to 2020',fontsize=18, y=1.02,weight='bold') plt.legend(loc='upper right') plt.legend(loc='upper left',bbox_to_anchor=(1, 1)) plt.show() Look at colors in box-plot # Airbnb median rental price across boroughs in 2019 plt.figure(figsize=(15,9)) themes.theme_ucberkeley(scheme=\"all\") df = newdata_2020[np.abs(newdata_2020.price-newdata_2020.price.mean())\u003c=(4*newdata_2020.price.std())] # Remove records with listing prices that are more than 3 standard deviations from the mean sns.catplot(x='borough', y=\"price\", kind=\"boxen\", data=df, palette = self_palette, height=5, # make the plot 5 units high aspect=2.5) # height should be 3 times width plt.xlabel('borough',fontsize=12) plt.ylabel('Listing Price',fontsize=12) plt.title('Rental price across neighborhoods',fontsize=18, y=1.2, weight='bold') plt.show() And if we want to do multiple histgram in the same plot plt.figure(figsize=(12,6)) themes.theme_ucberkeley(scheme=\"all\") b1 = newdata_2020.loc[newdata_2020['borough'] == 'Manhattan'] b2 = newdata_2020.loc[newdata_2020['borough'] == 'Bronx'] b3 = newdata_2020.loc[newdata_2020['borough'] == 'Brooklyn'] b4 = newdata_2020.loc[newdata_2020['borough'] == 'Queens'] b5 = newdata_2020.loc[newdata_2020['borough'] == 'Staten Island'] g=sns.distplot(b1[['price']], hist=False, rug=False, kde_kws={ 'color':'#07335d', \"lw\": 3, \"label\": \"Manhattan\"}) sns.distplot(b2[['price']], hist=False, rug=False,color='#ecd1cb', kde_kws={ \"lw\": 3, \"label\": \"Bronx\"}) sns.distplot(b3[['price']], hist=False, rug=False,color = '#56bdff', kde_kws={ \"lw\": 3, \"label\": \"Brooklyn\"}) sns.distplot(b4[['price']], hist=False, rug=False,color='#ff9856', kde_kws={ \"lw\": 3, \"label\": \"Queens\"}) sns.distplot(b5[['price']], hist=False, rug=False,color='#a9aaab', kde_kws={ \"lw\": 3, \"label\": \"Staten Island\"}) plt.tick_params( axis='y', # changes apply to the x-axis which='both', # both major and minor ticks","date":"2020-03-18","objectID":"/2020-03-18-change-category-colour-across-plots/:0:0","tags":["seaborn","visualization","python"],"title":"Change category colour across plots in Seaborn","uri":"/2020-03-18-change-category-colour-across-plots/"},{"categories":["Statistical modeling"],"content":"Risk of a large sample size Typically we don’t worry about sample size being too large, because that means we have more power, we can detect smaller effect with more significance. It’s not a problem because of technical reasons but human motivation and interpretations. ","date":"2020-03-02","objectID":"/2020-03-02-sample-size-reflection/:1:0","tags":["causal inference","sample size"],"title":"What happened if the sample size is very large?","uri":"/2020-03-02-sample-size-reflection/"},{"categories":["Statistical modeling"],"content":"Warning: Huge Samples Can Make the Insignificant…Significant When sample size is too large, unless the effect is truly not there, unless the coefficient is truly zero, it doesn’t matter how small the beta is, we will find it significant. If we have sufficient data, even the smallest possible effect will be captured. (A insignificant result will be significant if we multiply sample size by 10). ","date":"2020-03-02","objectID":"/2020-03-02-sample-size-reflection/:1:1","tags":["causal inference","sample size"],"title":"What happened if the sample size is very large?","uri":"/2020-03-02-sample-size-reflection/"},{"categories":["Statistical modeling"],"content":"Risk of that People put too much intention on p-value / significance (the number of stars) without and forget whether the coefficient/magnitude of coefficient is meaningful, practically significant, or how large / small the effect is (the value of coefficient). It is hinten on the issue of large sample size. At the end of the day, when we implement the causal inference solution, it may not improve the outcome by that much because we are chasing something mynute. ","date":"2020-03-02","objectID":"/2020-03-02-sample-size-reflection/:1:2","tags":["causal inference","sample size"],"title":"What happened if the sample size is very large?","uri":"/2020-03-02-sample-size-reflection/"},{"categories":["Statistical modeling"],"content":"Solution Always report not only the significance level but also what exactly the estimate is. ","date":"2020-03-02","objectID":"/2020-03-02-sample-size-reflection/:1:3","tags":["causal inference","sample size"],"title":"What happened if the sample size is very large?","uri":"/2020-03-02-sample-size-reflection/"},{"categories":["Statistical modeling"],"content":"Example Look at the 2-sample t-test results shown below. Notice that in both Examples 1 and 2 the means and the difference between them are nearly identical. The variability (StDev) is also fairly similar. But the sample sizes and the p-values differ dramatically: ","date":"2020-03-02","objectID":"/2020-03-02-sample-size-reflection/:1:4","tags":["causal inference","sample size"],"title":"What happened if the sample size is very large?","uri":"/2020-03-02-sample-size-reflection/"},{"categories":["Statistical modeling"],"content":"Power and sample size calculation for t test in R We know how to compute power and determine sample size for Normal (z) tests and confidence intervals. It’s a bit harder to do that by hand for a t distribution, but there is a powerful R function we can use, called power.t.test() that makes it easy. To use it, we need to know (or at least guess) a few things. We can give R values for all but one of several quantities, and R can determine the missing one for us. Suppose we want to know the power that a t-test has for detecting a difference as large as 1 unit from zero if the standard deviation is 3 units, we have a sample of n = 20, and we are testing with alpha = .05 power.t.test( 20 , 1 , 3 , .05 , NULL , type = \"one.sample\" ) One-sample t test power calculation n = 20 delta = 1 sd = 3 sig.level = 0.05 power = 0.2931601 alternative = two.sided So we have only a 29% chance of detecting en effect that size. How large a sample would we need to have power .8? power.t.test( NULL , 1 , 3 , .05 , .8 , type = \"one.sample\" ) One-sample t test power calculation n = 72.58407 delta = 1 sd = 3 sig.level = 0.05 power = 0.8 alternative = two.sided We need 73 (round up) to be reduce the Type II error risk to 20%. How large an effect could we detect with 80% power with out original sample size? power.t.test( 20 , NULL , 3 , .05 , .8 , type = \"one.sample\" ) One-sample t test power calculation n = 20 delta = 1.981323 sd = 3 sig.level = 0.05 power = 0.8 alternative = two.sided About 2 units (2/3 of a standard deviation). It also works for a two-sample problem; the default is actually the two-sample case. Suppose we want to detect a 1-unit difference in means between groups with standard deviation 3 in both groups, again we assume alpha = .05 and want power .8. power.t.test( NULL , 1 , 3 , .05 , .8 ) Two-sample t test power calculation n = 142.2466 delta = 1 sd = 3 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number in *each* group We can detect effects no smaller than about .9 standard deviations under those conditions. ","date":"2020-03-02","objectID":"/2020-03-02-sample-size-reflection/:2:0","tags":["causal inference","sample size"],"title":"What happened if the sample size is very large?","uri":"/2020-03-02-sample-size-reflection/"},{"categories":["Statistical modeling"],"content":"Introduction ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:1:0","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Unfair Competitive Advantage When watching competitive swimming, it seems like the swimmers in the middle lanes always win the race. Although this is partly by design, as the fastest qualifiers are placed in the innermost lanes, there is great debate amongst the swimming world whether there is additional advantage unintentionally given to these swimmers. In this project, we hope to address whether lane placement leads to an unfair competitive advantage. To illustrate this notion, take the example of Michael Phelps, widely regarded as the greatest swimmer of all time. In the 2012 Olympic Games, Phelps barely qualified for the finals of the 400 IM. As the slowest qualifier, he was placed in one of the outermost positions, lane 1. Unphased, Phelps didn’t think much of it. “The only thing that matters is getting a spot” he rationalized, “you can’t get a gold medal from the morning.” In retrospect, it’s understandable that Phelps was this confident despite having the slowest prelims time amongst qualifiers; he was, after all, the reigning gold medalist and world record holder in the event. However, he was not able to defend his title when swimming from an outside lane. As a matter of fact, he failed to medal at all, placing 4th in the event and adding over 5 seconds to his time from the prior Olympics. To put this in perspective, out of the 30 Olympic events that Phelps swam, the only other time he didn’t medal was at the age of 15. Although this example gives some credence to the claim that lane placement may affect performance, it is quite anecdotal. In our study, we will use real competition data to assess the effect of lane placement on performance. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:1:1","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Possible Reasons of the Effect Before we begin our analysis, it is important to establish some intuition as to why an effect may be plausible. First, there are physical reasons. Swimming in an outside lane typically means swimming with a wall directly to one side of you. When waves form, they crash off these walls and can cause turbulence or currents in the water with the greatest impact on those closest to the wall. Next, there are psychological factors at play. Being placed in a center lane implies that you are a favorite to win the heat whereas swimmers in the outside lanes are expected to finish last. Mentally, this could go two ways. Lastly, there are visual reasons that lane advantages could exist. Because they are swimming against a wall, swimmers in outside lanes do not have a clear view of their competition. As a matter of fact, they can only see competitors to one side of them and are multiple lanes away from the top seeded swimmers. In contrast, swimmers in the inside lanes can see other top competitors right next to them on either side. It’s possible that this could affect pacing significantly. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:1:2","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Ideal Experiment In order to determine the effect of lane placement conclusively, we would ideally conduct an experiment. This would involve gathering a sufficiently large group of swimmers, randomly assigning them to inside or outside lanes, subjecting both groups to the exact same preparation (e.g. bedtime, wake-up time, meals, warm-up, etc.), and having the swimmers race, recording the times. Then, results of the swimmers in inside and outside lanes could be compared to estimate an effect. Alas, we had neither the time nor the resources to conduct a proper experiment. Instead, we will be performing an observational study. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:1:3","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Data Overview The data used for our analysis comprises of meet results from conference championship meets in the California Community College Athletic Association between 2017 and 2019, scraped from SwimPhone.com. Each meet included in our dataset took place at the end of the season in either April or May, was swam in an 8 lane pool, and follows the same championship meet format with prelims of an event in the morning and the top 16 qualifiers racing again in the evening for finals. To simplify our analysis, we restrict the scope to events of 100 yards, the most common distance. Each unit of observation is a finalist in a specific event. The dependent variable we will be modeling is the finals time while the treatment is the lane placement in finals. Lanes 4 and 5 are considered inside lanes while lanes 1 and 8 are considered outside lanes. Other covariates included are the prelims time, the improvement from seed time to prelims time, the gender of the swimmer, and the stroke of the event. Figure 1: Continuous Data Distributions When examining the distributions of the time variables (Figure 1), there are several characteristics of note. It appears that both prelims time and finals time follow similar right skewed distributions. Most of the data is around 60 seconds with a few observations reaching 80 or even over 100 seconds but no extreme outliers. Additionally, the finals time seems to be slightly more concentrated at lower values. The time improvement from seed to prelims is centered above 0, indicating that most finalists improved upon their in-season seed time in prelims. This distribution is fairly symmetric with some outliers gaining more than 10 seconds or improving by over 20 seconds. Figure 2: Discrete Data Distributions Looking at the distributions of categorical variables (Figure 2), we see some interesting characteristics. As the data collected consists of the 100 yard races of each stroke across 22 meets, it is unsurprising that gender is quite evenly distributed. However, the distributions of stroke and lane are somewhat unexpected. Because there was an equal number of events scraped for each stroke, we would expect a balanced number of observations for each stroke. The dominance of freestyle in our dataset implies that swimmers were more likely to scratch out of finals in non-free strokes or some events did not have enough participants to fill two finals heats. Similar rationale might explain why our dataset is weighted more heavily towards inside lanes. Those who are expected to finish last in their heat may be more likely to scratch the event or, since lanes are seeded progressively slower towards the outside, events without enough participants for two full heats would result in empty outside lanes. Figure 3: Relationships Amongst Variables To better understand the variables that will be used in our analysis, we examined some relationships present (Figure 3). Intuitively, a swimmer’s performance in prelims is a very strong indicator of his or her performance in finals, as indicated by a nearly perfect correlation between the two times. In addition, we see that male swimmers’ times are typically faster and less varied than those of female swimmers. In terms of strokes, it’s evident that freestyle is the fastest stroke with the least variance, followed by butterfly, backstroke, and breaststroke, in that order. Now that we have a clearer picture of the data that we will be using, we can conduct our analyses. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:2:0","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Main Analysis ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:3:0","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Naïve Regression Before applying any econometric techniques to make causal inference in an observational setting, we ran a naïve regression model regressing finals time on prelims time, stroke, gender, time improvement, and lane. However, as discussed previously, lane placement for finals is not randomly assigned and there is clear endogeneity present. Therefore, we are unable to imply causality in our interpretation of the treatment effect. Results of this regression are shown below (Table 1). To be concise, insignificant predictors are excluded from this discussion. Variable Estimate p-value Prelims Time .9847 \u003c 2e-16 Improvement .1447 \u003c 2e-16 Lane (Treatment) .2830 .000215 Table 1: Naïve Regression Results We observe that for a one second increase in a swimmer’s prelims time, we would expect his or her finals time to increase by .9847 seconds, holding all other variables constant. Unsurprisingly, this indicates that swimmers are typically faster in finals heats than in prelims heats. Next, we can see that for each additional second of improvement between seed time and prelims time, we expect an increase in finals time by .1447 seconds, holding all else constant. Again, this makes sense. Swimmers who have improved significantly from their seed time likely do not have more effort to give for further improvement, whereas those who did not improve greatly from their in-season time may have more to give in finals. Next, we observe that swimmers in outside lanes swim, on average, .2830 seconds slower in finals than those in inside lanes, all else equal. Figure 4: Covariate Imbalance Before Matching As alluded to before, since treatment groups are not randomly assigned, we see significant differences between the two groups (Figure 4). Because lane placement is determined by prelims time, we see that swimmers in inside lanes are an average of around 5 seconds faster than those in outside lanes. Additionally, those in outside lanes had an average improvement in prelims of about half a second more than those in inside lanes. Last but not least, if the groups were truly similar, they should have a nearly identical distribution of propensity scores. A propensity score is a measure of the predicted probability of being placed in an outside lane based on all other factors. Intuitively, it can be thought of as a measure encompassing all covariates such that two observations with the same propensity score would have virtually the same characteristics. As we can see in the graphs above, it’s clear that this observational data cannot be treated like a randomized controlled trial. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:3:1","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Propensity Score Matching To address this non-random lane assignment, we employed an econometric technique called propensity score matching. This method finds swimmers in outside lanes with very similar propensity scores to swimmers placed in inside lanes and matches them. Those without a good match are discarded. The resulting dataset should eliminate covariate imbalance and recover treatment assignment as good as random, post hoc. After matching, it is evident that the characteristics of swimmers in outside and inside lanes are much more similar. Statistical tests confirm that there are not significant differences between the two groups in prelims time or time improvement. Moreover, we can see that the distribution of propensity scores is essential identical between those placed in inside or outside lanes. Now that we have obtained a dataset that simulates a randomly assigned treatment, we can refit a regression model of finals time based on prelims time, stroke, gender, time improvement, and lane placement. Results of this model are shown below (Table 2). Variable Estimate p-value Prelims Time .9945 \u003c 2e-16 Improvement .1347 1.25e-11 Lane (Treatment) .3342 .000114 Table 2: Matched Regression Results The significant predictors and estimates are quite similar to those before matching. Here, we see that for each additional second of time in prelims, we expect the time in finals to increase by .9945 seconds, holding all other variables constant. Again, we observe an expected increase in finals time of .1347 seconds for each incremental second of improvement from seed to prelims while controlling other predictors. Finally, we are able to estimate the effect of lane placement. All else equal, we estimate that swimming in an outside lane increases time by .3342 seconds. For reference, the difference between 1st place and 4th place of the men’s 100 meter freestyle in the 2016 Olympics was .30 seconds. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:3:2","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Conclusion ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:4:0","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Policy Implications Recall the three reasons we may see an effect of lane placement upon performance (physical, psychological, visual). Barring a radical redesign of the competition pool, we can only address two of these potential reasons: physical and mental factors. To combat the physical disadvantages of swimming in an outside lane, we first recommend ensuring that high level competitions have buffer lanes on the outside. This is already done in many top competitions. By having an extra lane against the wall, competitors will no longer be subject to waves crashing against the wall. In addition, there are a couple pool design characteristics that should be considered. There are gutters and lane lines that are specifically designed to disperse turbulence in the water. Investment in these technologies can help ensure that nobody is given an unfair advantage. Also, pools of greater depth reduce the likelihood of currents forming. This should be taken into consideration when building new competition pools. To address psychological factors that impair swimmers in the outside lanes, we propose random assignment of lanes to finalists. This could help eliminate the stigma and expectation of losing the race simply because of lane placement. ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:4:1","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Limitations There are several threats to causal inference that may bias our estimates. First, there is significant selection bias as we use a convenience sample. Since we are simply using data that was easily accessible, we do not have a random sample and it may not necessarily be representative of all competitive swimmers. Because of this, our findings may not generalize. Next, the key assumption of matching is that lane placement is fully determined by observed variables. Although lane placement is directly based on prelims time, there are a number of omitted variables that may affect both finals time and treatment. Unobservable characteristics such as energy level, motivation, and strategic decisions may all influence performance in prelims (and therefore, lane placement) and finals. Another variable we do not capture is the quality of competition swimsuit. Swimmers who are confident they will qualify for finals may save a faster suit for finals, biasing their speed in prelims and lane placement. Lastly, amount of competitive experience and age may be related to lane placement and performance. From a more technical perspective, the functional form of logistic regression used for matching is quite arbitrary. However, sensitivity analyses varying functional form, matching algorithm, parameter values, and model specifications show our results are quite robust. Finally, we are unable to account for differing lanes in the prelims swim. When including this as a covariate, we would essentially be estimating the treatment effect of lane placement twice. This would lead to a difficult to interpret and fairly unreliable estimate. Reference Various articles referenced in the introduction section. https://www.wsj.com/articles/did-the-olympic-pool-give-some-swimmers-an-advantage-1471470741 https://www.washingtonpost.com/news/wonk/wp/2016/09/01/these-charts-clearly-show-how-some-olympic-swimmers-may-have-gotten-an-unfair-advantage/ https://qz.com/761280/researchers-believe-certain-lanes-in-the-olympic-pool-may-have-given-some-swimmers-an-advantage/ http://www.bbc.co.uk/newsbeat/article/37083059/are-there-lane-advantages-in-athletics-swimming-and-track-cycling All data used for analysis was retrieved via SwimPhone.com from CCCAA meets. https://www.swimphone.com/ https://www.cccaasports.org Olympic swimming data was provided by Sports Reference. https://www.sports-reference.com/olympics ","date":"2020-02-23","objectID":"/2020-02-23-swimmer-performance-vs-lane-placement/:4:2","tags":["causal inference","matching"],"title":"Swimmer Performance vs Lane Placement Analysis","uri":"/2020-02-23-swimmer-performance-vs-lane-placement/"},{"categories":["Statistical modeling"],"content":"Summary ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:0","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Background Star Digital, a large multichannel video service provider, spends a large portion of their budget on advertising. As the technological environment changed, so did Star Digital’s advertising strategy as they began to invest more heavily in online advertising such as banner ads. In order to get the most out of their budget, they actively evaluate the return on investment (RoI) of each ad medium. In evaluating the effectiveness of an online advertising campaign, Star Digital has designed a controlled experiment. Subjects were randomly assigned to a treatment group where they receive Star Digital ads or a control group where they receive ab ad for a charity on a selection of sites on an ad-serving software. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:1","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Business Question Using the results of this experiment, Star Digital hopes to assess the causal effect of display advertising on sales conversion. In specific, there are three main questions they wish to address: Is online advertising effective for Star Digital? Is there a frequency effect of advertising on purchase? Which sites should Star Digital advertise on? ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:2","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Analysis Performed With regards to the first question, we performed a logistic regression to examine the influence of online advertising on purchases at Star Digital using test group as the regressor and conversion as the response. For the second question, we summed all impressions from the six sites and determined the impact of an increase in ad impressions on purchase via a logistic regression model adding total ad impressions as another regressor and its interaction. To address the last question, we defined an RoI metric in order to compare the effectiveness of sites 1 through 5 and site 6 and used logistic regression to estimate the effect of ad impressions from specific sites upon purchasing decision. We add replace total impressions with impressions from sites 1 to 5 and 6 and their interaction as regressors. We calculate RoI using our estimated causal impact per impression on purchase to determine the most profitable site decision for Star Digital. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:3","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Main Take-aways A. We do not have enough evidence to conclude that online advertisement is effective for Star Digital if the number of impressions is not factored in. Whether customers are in the treatment or control group does not significantly affect the odds of purchase at Star Digital. B. The total number of ad impressions online for consumers significantly influences whether they make a purchase with Star Digital or not. The odds of purchasing at Star Digital increase per ad impression seen for both the treatment and control group, but by a significantly bigger margin for those who saw Star Digital ads. This implies that users who are more active online are more likely to make a purchase in general and seeing Star Digital ads increases that probability moreover. C. The total number of impressions from site 1 through 5 have significant effects on the purchasing decision regardless of test group while impressions from site 6 do not have a significant effect. However, each incremental impression on either sites 1 to 5 or site 6 increases the probability of purchasing significantly more for the group exposed to Star Digital ads than the control group. D. Based on the ROI we recommend Star Digital invest its advertising budget into site 6 rather than sites 1-5. More details in my github repo. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:4","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Sample Size Analysis Now, we will discuss the implications of our sample size for the conclusions we can make. Because the sizes of the treatment and control groups are not even, we cannot use the same power test detailed in class as our sample violates the assumption that they are of equal size. Instead, we will use a test in the pwr package designed for unequal groups. 1 minus power refers to the probability of failing to detect an effect that exists. Therefore, a higher power reduces the probability that a real effect is undetected. ptab2 = cbind(NULL, NULL) for (i in c(.7, .75, .8, .85, .9, .95)){ pwrt = pwr.t2n.test(n1 = 2656, n2 = 22647, sig.level = .05, power = i, alternative = \"two.sided\") ptab2 = rbind(ptab2, cbind(pwrt$power, pwrt$d)) } plot(ptab2[,1], ptab2[,2], type = \"b\", xlab = \"Power\", ylab = \"Effect Size\") As we can see in the graph above, given our sample size and a standard significance level of 0.05, there is an increasing relationship between the level of power and the minimum effect size that can be reliably detected. In otherwords, at higher levels of power, there must be a larger effect to be detected reliable. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:5","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Randomization Check Finally, we will perform t-tests to check if the randomization between the treatment and control groups was done sufficiently and that the groups are roughly homogenous outside of the treatment. If the groups are significantly different, the conclusions of our analyses may not be reliable. t.test(imp_1 ~ test, data = star) # Output of tests for sites 2 to 5 are hidden to reduce redundancy as results are # similar to that of site 1 (sites 3/5 significant, sites 2/4 insignificant) invisible(t.test(imp_2 ~ test, data = star)) invisible(t.test(imp_3 ~ test, data = star)) invisible(t.test(imp_4 ~ test, data = star)) invisible(t.test(imp_5 ~ test, data = star)) t.test(imp_6 ~ test, data = star) t.test(sum1to5 ~ test, data = star) Welch Two Sample t-test data: imp_1 by test t = -3.905, df = 3574.1, p-value = 9.596e-05 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.5961772 -0.1976257 sample estimates: mean in group 0 mean in group 1 0.5756777 0.9725791 Welch Two Sample t-test data: imp_6 by test t = 0.43156, df = 2898.4, p-value = 0.6661 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.3176712 0.4969729 sample estimates: mean in group 0 mean in group 1 1.863705 1.774054 Welch Two Sample t-test data: sum1to5 by test t = -0.071371, df = 3268.6, p-value = 0.9431 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.8402427 0.7812196 sample estimates: mean in group 0 mean in group 1 6.065512 6.095024 When we consider websites 1 to 5 on their own, there seems to be statistically significant difference between the treatment and control groups. This implies that the composition of people in the two groups are not necessarily homogenous. However, when it comes to the total number of impressions seen in sites 1 to 5, given their interchangeability and Star Digital’s inability to show adds on a specific site, there is not enough evidence to say that the groups are significantly different. Similarly, in terms of their impressions on site 6, there is not enough evidence to assert that the treatment and control groups are not homogenous. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:1:6","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Main Analysis Now that we better understand the data we will be analyzing, we can move onto answering the three main questions that Star Digital has. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:2:0","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Question 1: Is online advertising effective for Star Digital? lm1 = glm(purchase ~ test, data = star, family = 'binomial') summary(lm1) exp(coef(lm1)) # This transformation of fitted model coefficients is done here and in further analyses # so that they can be interpreted as the odds ratio. Call: glm(formula = purchase ~ test, family = \"binomial\", data = star) Deviance Residuals: Min 1Q Median 3Q Max -1.186 -1.186 1.169 1.169 1.202 Coefficients: Estimate Std. Error z value Pr(\u003e|z|) (Intercept) -0.05724 0.03882 -1.474 0.1404 test 0.07676 0.04104 1.871 0.0614 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 35077 on 25302 degrees of freedom Residual deviance: 35073 on 25301 degrees of freedom AIC: 35077 Number of Fisher Scoring iterations: 3 (Intercept) test 0.9443631 1.0797852 Interpretations First, we will use logistic regression to determine whether being in the group that receives Star Digital ads has a significant increase in the odds of purchase. In the regression, the p-value for ‘test’ is 0.0614, which is greater than the acceptable significance level 0.05. This means there is not enough evidence to say whether the consumer is in the treatment or control group has an effect on whether he/she will eventually make a purchase at Star Digital. Hence, we do not have enough evidence to conclude that simply displaying online advertising is effective in stimulating purchases. Although we cannot confidently say that it is different from 0, we estimate that being part of the test group would increase the odds of purchasing Star Digital by 7.98%. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:2:1","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Question 2: Is there a frequency effect of advertising on purchase? In particular, the question is whether increasing the frequency of advertising increases the probability of purchase? star$total_imp = rowSums(star[,4:9]) lm2 = glm(purchase ~ test + total_imp + test*total_imp, data = star, family = 'binomial') summary(lm2) exp(coef(lm2)) Call: glm(formula = purchase ~ test + total_imp + test * total_imp, family = \"binomial\", data = star) Deviance Residuals: Min 1Q Median 3Q Max -4.9145 -1.1266 0.1299 1.2156 1.2433 Coefficients: Estimate Std. Error z value Pr(\u003e|z|) (Intercept) -0.169577 0.042895 -3.953 7.71e-05 *** test -0.013903 0.045613 -0.305 0.761 total_imp 0.015889 0.002876 5.524 3.32e-08 *** test:total_imp 0.015466 0.003207 4.823 1.42e-06 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 35077 on 25302 degrees of freedom Residual deviance: 34190 on 25299 degrees of freedom AIC: 34198 Number of Fisher Scoring iterations: 5 (Intercept) test total_imp test:total_imp 0.8440214 0.9861932 1.0160156 1.0155865 Interpretations To answer this question, we first create a new variable summing all the impressions from the websites. Now, we need to determine the impact of ad frequency on purchase. Again, we will use logistic regression. Our results show that simply being subject to the Star Digital ads does not result in a significant increase in purchasing. This supports the conclusion we reached in Question 1. Looking at the effect of total impressions on the odds of purchase, we see a very signifcant p-value, much lower than 0.05. This means that there is evidence that the total number of ad impressions for each consumer effects whether they make a purchase at Star Digital or not. The coefficent of the total impression term is 0.0159, which means that there is around a 1.6% increase in the odds of making purchasing at Star Digital for for each additional ad impression in the control group. This implies that more online activity increases the likelihood of purchasing from Star Digital, regardless of whether they are seeing Star Digital ads. Now focusing on the treatment group, we again see p-value for the interaction between being in the treatment group and total impressions is well below than 0.05. This indicates a significant difference in the effect of an additional ad impression between the treatment and control group. Examining the coefficient on the interaction term, we see that above and beyond the 1.6% increase in purchase odds for the control group, consumers in the treatment group are expected to have an additional 1.5% increase in odds of purchasing from Star Digital for each ad impression. As we can see, it appears that a higher frequency of advertising does increase the probability of purchase. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:2:2","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Question 3: Which sites should Star Digital advertise on? In particular, should it put its advertising dollars in site 6 or in sites 1 through 5? To address this question, we must first define a RoI metric in order to compare the effectiveness of sites 1 through 5 and site 6. To do so, we used the following formula as our criteria to assess the profitability of different sites: ROI = ((Value of Purchase * Increase in Odds of Purchase) - Cost of Impression) / Cost of Impression lm3 = glm(purchase ~ test + sum1to5 + imp_6 + test*sum1to5 + test*imp_6, data = star, family = 'binomial') summary(lm3) exp(coef(lm3)) Call: glm(formula = purchase ~ test + sum1to5 + imp_6 + test * sum1to5 + test * imp_6, family = \"binomial\", data = star) Deviance Residuals: Min 1Q Median 3Q Max -5.1280 -1.1195 0.1185 1.2217 1.2472 Coefficients: Estimate Std. Error z value Pr(\u003e|z|) (Intercept) -0.166556 0.042533 -3.916 9.01e-05 *** test -0.006087 0.045314 -0.134 0.893139 sum1to5 0.019452 0.003443 5.650 1.61e-08 *** imp_6 0.003978 0.004294 0.927 0.354179 test:sum1to5 0.014617 0.003794 3.852 0.000117 *** test:imp_6 0.013483 0.005405 2.494 0.012616 * --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 35077 on 25302 degrees of freedom Residual deviance: 34166 on 25297 degrees of freedom AIC: 34178 Number of Fisher Scoring iterations: 5 (Intercept) test sum1to5 imp_6 test:sum1to5 test:imp_6 0.8465755 0.9939313 1.0196422 1.0039860 1.0147240 1.0135741 Interpretations Our final regression summary using logistic regression and assessing the effect of ads on certain websites again confirms our results from Question 1 that just being in the treatment or control group does not significantly affect the odds to purchase. Additionally, the total impressions from site 1 through 5 has a significant effect on the odds of purchase purchase while there is not evidence to say that number of impressions from site 6 has a significant impact at the 95% confidence level. More specifically, we estimate that for each additional impression on sites 1 through 5, the odds of purchasing from Star Digital increases by 1.96%. However, each additional impression on site 6 leads to an expected increase in the odds of purchase by only 0.40%. In addition, the p-value for interaction terms on being in the treatment group and the total number of impressions on each group of sites is very small and we have evidence to suggest that the treatment group’s purchase behavior changes differently with additional impressions than the treatment group. Compared to the control group, each additional impression from sites 1 to 5 for the treatment group is expected to have a 1.47% increase in purchase odds more than the 1.96% increase expected in the control group. Similarly, those in the treatment group are expected to have a 1.36% higher increase in odds of purchase for each additional site 6 ad than those in the control group. Meanwhile, site 1 to 5 runs ads at 25 dollars per 1000 impressions whereas site 6 runs ads at 20 dollars per 1000 impressions. Therefore, our ROI is calculated as follows: ROI_site1to5 = ((1200 * .0147240) - (25 / 1000)) / (25 / 1000) ROI_site1to5 ROI_site6 = ((1200 * .0135741) - (20 / 1000)) / (20/1000) ROI_site6 [1] 705.752 [1] 813.446 As we can see, there is a higher ROI in investing in ads on site 6 and we conclude that Star Digital should put its advertising dollars in site 6. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:2:3","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Concerns and Limitations Now that our analysis is complete, we will briefly touch on some of the major threats and limitations of our results and conclusions. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:3:0","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Threats to Causal Inference First, we will discuss the four key threats to causal inference and the extent to which they are present in this experiment. Selection Bias: There appears to be a large threat of selection bias pressent in this experiment. Since the group of consumers participating in the experiment are not described, we do not know if they are representative of all potential customers that they would advertise to on these sites. In addition, the sample that is used for analysis is a choice-based sample with about one half of subjects resulting in purchases. Because of this design choice, we would need to do a transformation to map it back to the population in order to make proper inference and interpretations. Interpretations such as the increase in odds of purchase and RoI are likely skewed higher than they should be since the true conversion rate was 0.153% rather than the 50% represented in the sample. Omitted Variable Bias: There are no demographic information of users is provided. For example, if variables such as gender or age is correlated to both the time spent on surfing websites and probability of subscribing the package, then those omitted bias will cause endogeneity of the experiment. Although using a randomized experiment should control for this, we do not have enough information to conclude whether randomization controlled for this. In addition, as we saw in our randomization checks, the treatment and control groups do show differences in activity on individual websites 1 through 5. Depending on the the content and nature of these websites, these differences in activity may reflect latent differences in the characteristics of the treatment and control group that may be correlated with the purchase decision. Simultaneity Bias: It does not seem that simultaneity should be a threat to this as Star Digital assigned treatment and control and purchase decision should not determine the frequency of ads. Measurement Error: The measurement method of impressions may be problematic to some extent as we don’t know the amount of time spent with the ad visible or if the user actually looked at the ad. Additionally, if ads that are hidden by software such as an ad blocker are still recorded, we may have inaccurate measurements. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:3:1","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Limitations There are various other limitations in the experimental design that either hinder our conclusions or allow for more to be desired. First, it is unclear whether subject of the experiment are aware of the experiment. If they are, they may change their behavior or act in a way that they think is more desirable either socially or to the experimenter. Also, there may be some concern of interference as subjects could share information online which can undermine our ability to recover accurate causal estimates. Similarly, as this is the evaluation of a single ad campaign, users both in the treatment and control groups may have been inadvertently exposed to other Star Digital ads through either traditional media or a different online campaign during the experiment. Additionally, exposure to Star Digital advertisements could make people want to purchase the service in general, not only from Star Digital. It could make sense to use market share as another metric to examine. Finally, there is no information about conversion timing. If it’s a very long time, a conversion that occurred long after someone saw an ad may be counted as a view-through conversion although it’s not very plausible that the purchase actually stemmed from a banner ad seen long ago. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:3:2","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Key takeaways ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:4:0","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Interpretation of interaction effect (secondary effect) Interaction effect Interaction term is an effect on effect, not effect of a variable on another variable. Effect X(treatment) on Y: We believe that changing x by some amount, Y will change by some corresponding amount. Interaction effect: We believe the causal relationship between X and Y depends on the magnitude of Z. In this case, for question 2, we should use interaction effect even though the question is imbiguous to start with. I not only care about the effect of showing an advertising versus a charitable advertising on purchase, but also how does the strength of effect depends on how frequently I show you the advertising. Interaction vs subsample analysis What about we do a sub-sample regression? Just taking out the group that made purchase is somehow problematic. Then problem with it is that it is not a causal problem. We don’t need an expriment to get the result if that’s what we care about because we don’t care what is a baseline conversion. Think about the trick when business make people to buy fraudulent products. “I have developed a magic pill to cure common cold. If you take a pill, your symptoms will be significantly released within a week and if you take extra, you will recover even faster.” – I am not telling you what would happen without the pill, you will still get better even without any pills in a week. The pill does nothing, and yet, you will see a frequency change. In this case, maybe StarDigital ad is counterproductive, we will not see that if we don’t compare it with the baseline. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:4:1","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Baseline of treatment issue What if we omit $\\beta_1 X$ term? In this case, if we omit the X term, for one, there will be omit variable problem for sure. For two, it implies that we believe if we show zero advertising, the treatment effect of star digital ad equals zero, which is not a ridiculous hypothesis to make. But in practice, why make this assumption when we don’t have to. General practice, run the full-fledged regression. Put in the isolated treatment term ($\\beta_1 X$). If the baseline treatment effect is zero, $\\beta_1$ will turn out to be zero or closed to zero (self contained) and insignificant and we will get what we want. However, if we omit it, in the alternative world where we do not omit it and $\\beta_1$ is significant different from zero, we will be soooo wrong. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:4:2","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Logistic regression (Latent factor regression) coefficient Converting coefficients of Log odds back to probability can be a hassle. Log odds (actual dependent variable): log[P(purchase)/(1-P(purchase))] Describing changes in odds, not probability: One unit change in X, leads to one unit change in log odds. When we want to convert it back, we want a baseline conversion probability. In this particular case, we can get that conversion probability in the control group. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:4:3","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["Statistical modeling"],"content":"Marginal effect of logistic regression Decreasing margin of effect? We we observe the decreasing marginal effect from X on Y, means they don’t have linear relationship, then we might want to include X square term. ","date":"2020-02-17","objectID":"/2020-02-17-effectiveness-of-online-advertising/:4:4","tags":["statistical modeling","causal inference"],"title":"Effectiveness of Online Advertising","uri":"/2020-02-17-effectiveness-of-online-advertising/"},{"categories":["shower thought"],"content":"Honestly, I don’t care what we do tonight. We could go out and get drinks, or grab some beer and stay in. We could check out that new Japanese place we’ve been meaning to try, or just swing by that Mexican place by the river. We could make Chinese as always. We could get pizza on the patio. We could pound down a two-gallon bucket of tzatziki sauce in the back of the car. Burgers. Fries. Seriously, whatever. But do you even really want to see a movie? We could sit back and browse whatever, but no, I do not have an opinion, not today. Keep in mind, these are only simultaneously vague and highly-specific suggestions I feel lukewarmly about. I want to drink a little tonight but only if you want to, too, and if you don’t then me neither; Whiskey’s cool, I don’t even need a mixer. Should I pick up some Diet Coke or something while I’m at the store? I am an emotional ghost. I’m down to clown. I’m down to drown. I’m up for anything and everything or nothing. Okay, but when was the last time we sat down and talked? You know, really hashed it out. Told each other everything, no barriers. Isn’t that an activity in itself? Why go do “anything” when we have our own world, our own universe, right here. Or, why don’t we just be with ourselves instead? Hey, why so serious? Just an old shower thought I’m rambling about here. ","date":"2020-01-30","objectID":"/2020-01-30-im-fine-with-whatever/:0:0","tags":null,"title":"I'm fine with whatever","uri":"/2020-01-30-im-fine-with-whatever/"},{"categories":["Statistical modeling"],"content":"MSE Sensitive to outliers Has the same units as the response variable. Lower values of MSE indicate better fit. Actually, it’s hard to realize if our model is good or not by looking at the absolute values of MSE or MSE. We would probably want to measure how much our model is better than the constant baseline. ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:1:0","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"Disadvantage of MSE If we make a single very bad prediction, taking the square will make the error even worse and it may skew the metric towards overestimating the model’s badness. That is a particularly problematic behaviour if we have noisy data (data that for whatever reason is not entirely reliable)  On the other hand, if all the errors are smaller than 1, than it affects in the opposite direction: we may underestimate the model’s badness. ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:1:1","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"R-square proportional improvement in prediction of the regression model, compared to the mean model (model predicting all given samples as mean value). interpreted as the proportion of total variance that is explained by the model. relative measure of fit whereas MSE is an absolute measure of fit often easier to interpret since it doesn’t depend on the scale of the data. It doesn’t matter if the output values are very large or very small, always has values between -∞ and 1. There are situations in which a high R2 is not necessary or relevant. When the interest is in the relationship between variables, not in prediction, the R2 is less important ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:2:0","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"Caution of R-square For example: there is a linear regression: BMI ~ weight + height **(wrong model) ** R2 = 0.9 ln(BMI) ~ ln(weight) + ln(height) **(right model) ** R2 = 0.88 why? It’s not out of sample evaluation! We should withhold part of data to evaluation a model R square is methmetically flaw. $R^{2} = \\frac{\\sum (\\hat{y} – \\bar{\\hat{y}})^{2}}{\\sum (y – \\bar{y})^{2}}$ R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making σ2 large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular. What is σ2 ? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between “almost” and “exact” is assumed to be a draw from a Normal distribution with mean 0 and some variance we call σ2 . R-squared can be arbitrarily close to 1 when the model is totally wrong. Let’s recap: R-squared does not measure goodness of fit. R-squared does not measure predictive error. R-squared does not allow you to compare models using transformed responses. R-squared does not measure how one variable explains another. ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:2:1","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"MSE vs R-square $MSE$ measures how far the data are from the model’s predicted values $R^2$ measures how far the data are from the model’s predicted values compare to how far the data are from the mean The difference between how far the data are from the model’s predicted values and how far the data are from the mean is the improvement in prediction from the regression model. ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:2:2","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"RMSE and RMSLE ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:3:0","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"Mechanism It is the Root Mean Squared Error of the log-transformed predicted and log-transformed actual values. RMSLE adds 1 to both actual and predicted values before taking the natural logarithm to avoid taking the natural log of possible 0 (zero) values. As a result, the function can be used if actual or predicted have zero-valued elements. But this function is not appropriate if either is negative valued RMSLE measures the ratio of predicted and actual. $log(p_i +1) − log(a_i+1)$ can be written as 𝑙𝑜𝑔((𝑝𝑖+1)/(𝑎𝑖+1)) ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:3:1","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["Statistical modeling"],"content":"RMSLE is preferable when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc we care about percentage errors rather than the absolute value of errors. there is a wide range in the target variables we don’t want to penalize big differences when both the predicted and the actual are big numbers. we want to penalize under estimates more than over estimates. Lets have a look at the below example Case a) : Pi = 600, Ai = 1000 (under estimate) RMSE = 400, RMSLE = 0.5108 Case b) : Pi = 1400, Ai = 1000 (over estimate) RMSE = 400, RMSLE = 0.3365 As it is evident, the differences are same between actual and predicted in both the cases. RMSE treated them equally however RMSLE penalized the under estimate more than over estimate. MSE incorporates both the variance and the bias of the predictor. RMSE is the square root of MSE. In case of unbiased estimator, RMSE is just the square root of variance, which is actually Standard Deviation. In case of RMSLE, you take the log of the predictions and actual values. So basically, what changes is the variance that you are measuring. If both predicted and actual values are small: RMSE and RMSLE is same. If either predicted or the actual value is big: RMSE \u003e RMSLE If both predicted and actual values are big: RMSE \u003e RMSLE (RMSLE becomes almost negligible) Sources: ","date":"2020-01-20","objectID":"/2020-01-20-the-good-bad-ugly-of-r-square/:3:2","tags":["metric","causal inference"],"title":"The Good Bad Ugly of R-Square","uri":"/2020-01-20-the-good-bad-ugly-of-r-square/"},{"categories":["machine learning"],"content":"Binary Image Classification with Keras and Transfer Learning","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Image classification exercise I’m not gonna lie. I’m more of a dog person. Better know how to tell the difference. 😂 Here we go. ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:0:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Introduction This is an Image classification exercise. I will play with an expired Kaggle competition. The code is available in a jupyter notebook here. You will need to download the data from the Kaggle competition. https://www.kaggle.com/c/dogs-vs-cats/. The dataset contains 25,000 images of dogs and cats (12,500 from each class). I will create a new dataset containing 3 subsets, a training set with 16,000 images, a validation dataset with 4,500 images and a test set with 4,500 images. I will first build a vanilla CNN model as the baseline. And then apply transfter learning to further improve the performance. Keras comes prepackaged with many types of these pretrained models. Some of them are: VGGNET : Introduced by Simonyan and Zisserman in their 2014 paper, Very Deep Convolutional Networks for Large Scale Image Recognition. RESNET : First introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition INCEPTION: The “Inception” micro-architecture was first introduced by Szegedy et al. in their 2014 paper, Going Deeper with Convolutions: and more. Detailed explanation of some of these architectures can be found here. ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:1:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Start here import numpy as np import pandas as pd import os, shutil import random import keras from keras.preprocessing import image from keras.preprocessing.image import ImageDataGenerator from keras.utils import to_categorical from sklearn.model_selection import train_test_split import cv2 import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline %config InlineBackend.figure_format = 'retina' PATH = '../input/' train_dir = '../input/train/' test_dir = '../input/test/' ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:2:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Prepare data train_images = [] train_labels = [] for img in os.listdir(train_dir): try: img_r = cv2.imread(os.path.join(train_dir, img), cv2.IMREAD_COLOR) train_images.append(np.array(cv2.resize(img_r, (150, 150), interpolation = cv2.INTER_CUBIC))) if 'dog' in img: train_labels.append(1) else: train_labels.append(0) except Exception as e: print('broken image') broken image plt.title(train_labels[0]) plt.imshow(train_images[0]) train_df = pd.DataFrame({ 'train_images':train_images, 'train_labels':train_labels}) sns.set(style=\"darkgrid\") ax = sns.countplot(x=\"train_labels\", data = train_df,palette=\"Set3\") num_dogs = train_df['train_labels'].sum() print('We have {} cats and {} dogs to train with.'.format(train_df.shape[0]-num_dogs,num_dogs)) We have 12500 cats and 12500 dogs to train with. We have 12000 cats and 12000 dogs to train with, so the dataset is balanced. train_labels = np.array(train_labels) X_train, X_test, y_train, y_test = train_test_split(train_images, train_labels, test_size=0.2, random_state=13) X_train = np.array(X_train) X_test = np.array(X_test) print(\"Train Shape:{}\".format(X_train.shape)) print(\"Test Shape:{}\".format(X_test.shape)) Train Shape:(20000, 150, 150, 3) Test Shape:(5000, 150, 150, 3) X_train = X_train.reshape(-1, 150, 150, 3) X_test = X_test.reshape(-1, 150, 150, 3) print(\"Training data shape : {0}\".format(X_train.shape)) print(\"Training label shape : {0}\".format(y_train.shape)) print(\"Valid data shape : {0}\".format(X_test.shape)) print(\"Valid label shape : {0}\".format(y_test.shape)) Training data shape : (20000, 150, 150, 3) Training label shape : (20000,) Valid data shape : (5000, 150, 150, 3) Valid label shape : (5000,) ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:3:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"The base CNN model from keras.models import Sequential from keras import layers from keras import optimizers from keras import backend as K from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization from keras.applications import VGG16 # Creating the convnet model def build_cnn(): model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(128, (3, 3), activation='relu')) model.add(BatchNormalization()) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Dense(1, activation='sigmoid')) # 2 because we have cat and dog classes return model ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:4:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Fit the model model_base = build_cnn() model_base.name = 'base_cnn' model_base.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_1 (Conv2D) (None, 148, 148, 32) 896 _________________________________________________________________ batch_normalization_1 (Batch (None, 148, 148, 32) 128 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32) 0 _________________________________________________________________ dropout_1 (Dropout) (None, 74, 74, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 72, 72, 64) 18496 _________________________________________________________________ batch_normalization_2 (Batch (None, 72, 72, 64) 256 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64) 0 _________________________________________________________________ dropout_2 (Dropout) (None, 36, 36, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 34, 34, 128) 73856 _________________________________________________________________ batch_normalization_3 (Batch (None, 34, 34, 128) 512 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128) 0 _________________________________________________________________ dropout_3 (Dropout) (None, 17, 17, 128) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 36992) 0 _________________________________________________________________ dense_1 (Dense) (None, 512) 18940416 _________________________________________________________________ batch_normalization_4 (Batch (None, 512) 2048 _________________________________________________________________ dropout_4 (Dropout) (None, 512) 0 _________________________________________________________________ dense_2 (Dense) (None, 1) 513 ================================================================= Total params: 19,037,121 Trainable params: 19,035,649 Non-trainable params: 1,472 _________________________________________________________________ from keras.optimizers import RMSprop model_base.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics=['accuracy']) # model_base.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) history_base=model_base.fit(x=X_train, y=y_train, epochs=20, verbose=1,validation_data=(X_test, y_test)) model_base.save_weights('model_base_wieghts.h5') model_base.save('model_base.h5') ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:5:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Performance of base model loss = model_base.evaluate(x=X_test, y=y_test)[0] acc = model_base.evaluate(x=X_test, y=y_test)[1] print('Base CNN Model gives me LogLoss of {}'.format(loss)) print('Base CNN Model gives me Accuracy of {}'.format(acc)) 5000/5000 [==============================] - 2s 408us/step 5000/5000 [==============================] - 2s 336us/step Base CNN Model gives me LogLoss of 0.5596861797451973 Base CNN Model gives me Accuracy of 0.8514 acc = history_base.history['acc'] val_acc = history_base.history['val_acc'] loss = history_base.history['loss'] val_loss = history_base.history['val_loss'] epochs = range(1, len(acc)+1) fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,6)) ax1.plot(epochs, loss, color='blue', label = 'Training loss') ax1.plot(epochs, val_loss,color='green', label = 'Validation loss') ax1.set_title('Training and Validation Accuracy') ax1.legend() ax2.plot(epochs, acc, color='blue',label = 'Training acc') ax2.plot(epochs, val_acc,color='green',label = 'Validation acc') ax2.set_title('Training and Validation Accuracy') ax2.legend() plt.show() Obvious overfitting charastics and can’t achieve good accuracy score. The reason might be we have limited dataset even we already include data agumentation in the CNN model. To solve that, we can use transfer learning to take advantages of features from imagenet. There are two ways of using a pre-trained network: Running the convolutional base over our dataset, recording its output to a Numpy array on disk and then using this data as an input to a standalone, densely connected classifier Extending the model by adding a top layer and then training model The first method is cheap in terms of computation since it doesn’t require you to train the model from scratch. For the same reason we cannot take advantage of Data Augmentation The second method involves training the network from scratch. This will help us exploit image augmentation, but for the same reason it will be computationally expensive I will use the first type in this homework for the sake of time and computation ability as we have roughly enough data to train with. ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:5:1","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Transfer Learning VGG16 The loaded model is chopped after last max-pool layer in VGG16 architecture by setting the parameter include_top=false. # Defining and training the densely connected classifier def model_transfer_vgg16_chopped(): model = Sequential() #model.add(ResNet50(include_top=False, pooling=POOLING)) model.add(VGG16(include_top=False, weights='imagenet', input_shape=(150,150,3))) model.add(layers.Flatten()) model.add(layers.Dense(256, activation = 'relu')) model.add(layers.Dropout(0.5)) model.add(layers.Dense(1,activation = 'sigmoid')) model.layers[0].trainable = False model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['acc']) return model model_transfer = model_transfer_vgg16_chopped() model_transfer.summary() Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 58892288/58889256 [==============================] - 2s 0us/step _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= vgg16 (Model) (None, 4, 4, 512) 14714688 _________________________________________________________________ flatten_2 (Flatten) (None, 8192) 0 _________________________________________________________________ dense_3 (Dense) (None, 256) 2097408 _________________________________________________________________ dropout_5 (Dropout) (None, 256) 0 _________________________________________________________________ dense_4 (Dense) (None, 1) 257 ================================================================= Total params: 16,812,353 Trainable params: 2,097,665 Non-trainable params: 14,714,688 _________________________________________________________________ print(\"The number of trainable weights before freezing the convolutional layer = \",len(model_transfer.trainable_weights)) The number of trainable weights before freezing the convolutional layer = 4 # set callbacks from keras.callbacks import EarlyStopping, ReduceLROnPlateau earlystop = EarlyStopping(monitor='val_loss',patience=3) learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=0.0001) callbacks = [earlystop, learning_rate_reduction] history_vgg16 = model_transfer.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 20, callbacks=callbacks, verbose=1) ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:6:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Performance of transfer learning model model_transfer.evaluate(X_test, y_test) [0.27564358766500857, 0.9672] model_transfer.save_weights('model_transfer_weights.h5') model_transfer.save('model_transfer.h5') acc = history_vgg16.history['acc'] val_acc = history_vgg16.history['val_acc'] loss = history_vgg16.history['loss'] val_loss = history_vgg16.history['val_loss'] epochs = range(1, len(acc)+1) fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,6)) ax1.plot(epochs, loss,color='blue', label = 'Training loss') ax1.plot(epochs, val_loss,color='red', label = 'Validation loss') ax1.set_title('Training and Validation Log Loss') ax1.legend() ax2.plot(epochs, acc, color='blue',label = 'Training acc') ax2.plot(epochs, val_acc,color='red',label = 'Validation acc') ax2.set_title('Training and Validation Accuracy') ax2.legend() plt.show() ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:6:1","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["machine learning"],"content":"Test We will convert the predict category back into our generator classes by using train_generator.class_indices. It is the classes that image generator map while converting data into computer vision. test_images = [] test_filenames = [] for img in os.listdir(test_dir): try: img_r = cv2.imread(os.path.join(test_dir, img), cv2.IMREAD_COLOR) test_images.append(np.array(cv2.resize(img_r, (150, 150), interpolation=cv2.INTER_CUBIC))) test_filenames.append(img.split('.')[0]) except Exception as e: print('broken image') test_filenames = [int(a) for a in test_filenames] broken image test_images = np.array(test_images) test_images = test_images.reshape(-1, 150, 150, 3) predictions = model_transfer.predict(test_images) predictions2 = predictions.clip(min=0.005,max=0.995) results = pd.DataFrame({\"id\": test_filenames, \"label\":list(predictions2)}) results['label'] = results['label'].map(lambda x: str(x).lstrip('[').rstrip(']')).astype(float) results.to_csv('submission.csv', index=False) for i in range(0,10): if predictions2[i, 0] \u003e= 0.5: print('I am {:.3%} sure this is a Dog'.format(predictions[i][0])) else: print('I am {:.3%} sure this is a Cat'.format(1-predictions[i][0])) plt.imshow(test_images[i]) plt.show() I am 100.000% sure this is a Dog I am 100.000% sure this is a Cat I am 100.000% sure this is a Cat I am 100.000% sure this is a Cat I am 100.000% sure this is a Dog I am 100.000% sure this is a Dog codes available on GitHub ","date":"2019-11-28","objectID":"/2019-11-28-cat-or-dog-classification-dnn/:7:0","tags":["neural network","classification","transfer learning"],"title":"Cats vs Dogs Classification","uri":"/2019-11-28-cat-or-dog-classification-dnn/"},{"categories":["Machine learning"],"content":"1. Project Definition \u0026 Introduction When someone opens a restaurant, their focus is likely on making high-quality food that will make their customers happy. However, this does not cover all the problems they encounter. How do they effectively schedule the staff? How do they know the quantity of ingredients to order? If restaurants cannot solve these problems, their business will be hurt. If restaurants can predict how many visitors will be in one day, it’s easier for them to make the arrangement. However, forecasting the number of visits is hard because it might be influenced by countless factors (eg weather, holiday, and location). It’s even harder for new restaurants with little historical data to make accurate predictions. We are going to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be more efficient and allow them to focus on creating an enjoyable dining experience for their customers. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:1:0","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Data Sources The following information was taken straight from the Kaggle prompt: Our data comes from two separate sites and can be downloaded from Kaggle: Hot Pepper Gourmet (hpg): similar to Yelp, here users can search restaurants and also make a reservation online AirREGI / Restaurant Board (air): similar to Square, a reservation control and cash register system We use historical visits, reservations, and store information from both sites to forecast the daily number of visitors for certain restaurants on given dates. The training data covers the dates from 2016 until April 2017. The test set covers the last week of April and May of 2017. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the Golden Week.” Also there are days in both training and test set where the restaurant were closed and had no visitors. The training set omits days where the restaurants were closed, and in testing set, they are ignored in scoring. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:1:1","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"File Descriptions As mentioned above, there are datasets from two separate systems. Each file is prefaced with the source (eitherair or hpg) to indicate its origin. Each restaurant is associated with a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that we have been provided data beyond restaurants for which we make predictions on. Latitudes and Longitudes are not exact to discourage de-identification of restaurants. (The file description details are from the website introduction) air_reserve.csv This file contains reservations made in the air system. air_store_id - the restaurant’s id in the air system visit_datetime - the date and time of the visit for the reservation reserve_datetime - the date and time a reservation was made reserve_visitors - the number of visitors for that reservation hpg_reserve.csv This file contains reservations made in the hpg system. hpg_store_id - the restaurant’s id in the hpg system visit_datetime - the date and time of visit for the reservation reserve_datetime - the date and time a reservation was made reserve_visitors - the number of visitors for that reservation air_store_info.csv This file contains stores information in the air system. Column names and contents are self-explanatory. Note the latitude and longitude are geographical information of the area to which the store belongs. air_store_id, air_genre_name, air_area_name, latitude, longitude 4. hpg_store_info.csv This file contains stores information in the hpg system. Column names and contents are self-explanatory. Note: latitude and longitude are the latitude and longitude of the area to which the store belongs. hpg_store_id hpg_genre_name hpg_area_name latitude longitude store_id_relation.csv This file allows you to join select restaurants that have both the air and hpg system. ‌ air_visit_data.csv This file contains historical visit data for the air restaurants. air_store_id visit_date - the date visitors - the number of visitors to the restaurant on the date date_info.csv This file shows a submission in the correct format, including the days for which you must forecast. calendar_date day_of_week holiday_flg - is the day a holiday in Japan ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:1:2","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Model Summary Models Name ERROR(RMSLE) ARIMA 0.56137 PROPHET 0.54208 LIHGT GBM(LGBM) 0.52412 LSTM SEQ2SEQ 0.50277 LSTM + LGBM 0.50002 We started by using ARIMA and PROPHET as our baseline model. Both are typical models when facing time-series related problem. However, the typical model can’t take external data we have into account such as weather and store location. We decided to use other models that are capable of overcoming these challenges. We also decided to try some ensembling methods. Light GBM is a very popular model and also gives good performance. We also included LSTM which is another traditional model for time-series prediction. Finally, we got a RMSLE with 0.50002 with a combination of LSTM and LGBM. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:1:3","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Analysis Flow We are following a very standard machine learning process which can be concluded as below chart: ​ Source: hackernoon The first steps were to understand the main problem, get familiar with the structure of the data and decide what features we need. This is a Time-Series prediction, so we need to be careful about the sequence of the data. In any modeling process, the data that goes into a model plays a big role in ensuring accurate results. Therefore, the relevant features that helped achieve the objective were defined and an initial feature set was selected. Following this, some noisy and missing data was removed. We had access to data on restaurant reservations, but the dataset was incomplete and not of much use to us. Thus we didn’t take this factor into account. Too many missing values might lead to a worse result even if it’s a good predictor. Some fields’ values were imputed - for example, those of extreme actual visitor numbers on a specific day. After the data engineering, the core modeling process was started. Different algorithms were implemented on the feature set, along with cyclical addition and removal of features depending on performance and complexity of the features and the model used. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:1:4","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"2. Feature Engineering Because this is a time series problem, t’s better to take feature selection as the first step. Feature selection enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret and reduces the risk of overfitting. It also improves the accuracy of a model if the right subset is chosen. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:2:0","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Time Series Features In order to take advantage of the date information, we extract year/month/day/weekday from the column visit_date as our features. Visit date - original column in the data. The date visitor come to the restaurant. Used for mapping different data files Day of week - day of the week. Use numeric value to represent. If the day is weekend, it might have more traffic than normal weekdays. Month of the Year - different months also has different volume. This variable is kind of similar to season which showcase the seasonality of the time in a year Holiday Status - whether a day is holiday. This vairable can indicate the holiday flag. If it’s holiday time, there might be higher traffic outside and restaurant might also have more number of visitors. Thus this shoud be a good predictor Next day holiday - whether the next day is holiday. Days around holiday might also play a role in attracting visitors to restaurants. If the next day is holiday, people may be tried and don’t hand out in the previous Previous day holiday - Same to the above two predictors Consecutive Holidays - other than normal holiday flag data in the data_info.csv, we believe consective holidays and the length of days off-work also have a say in restaurant visiting patterns. For example, if the holiday is Friday, we will mark Friday, and the followed weekend with 3. Same goes when Monday is holiday and so on. Seasons - Japan has four distinct seasons: March to May is spring; June to August is summer; September to November is autumn; and December to February is winter. Each season has very different temperatures and climates which might affect the traffic. Prior Year Mapping One obvious solution to predicting visitors would be to look at how many customers came the year before. For example, if we are predicting for Jan 10, we might want to look at Jan 10th of the prior year. However, this is a slight problem here: what if Jan 10th of the previous year fell on a Saturday, but this year it falls on a Monday? To adjust this, we instead will take the number of visitors from the matching week of the year \u0026 the day of the week together. After running our LightGBM model, this feature was in the top 5 in terms of importance. final['prev_visitors'] = final.groupby( [final['visit_date'].dt.week, final['visit_date'].dt.weekday])['visitors'].shift() Days since 25th (Paycheck Day. Yay!) The next feature calculates how many days it has been since the previous 25th of the month. The 25th is special because this is when most Japanese people receive their monthly paycheck Japan Visa In a country like the US, this may not play too large of a role as people simply use a credit card. In Japan, however, people seem to be averse to debt and prefer cash over credit cards (Business in Japan). After running our LightGBM model, this feature was in the top 5 in terms of importance. def daysToPrev25th(row): TARGET_DATE = 25 if row['dayofmonth'] \u003e= 25: return row['dayofmonth'] - TARGET_DATE else: return row['daysinPrevmonth'] - TARGET_DATE + row['dayofmonth'] air_visit[\"dayofmonth\"] = air_visit[\"visit_date\"].dt.day air_visit[\"daysinPrevmonth\"] = (air_visit[\"visit_date\"] - pd.DateOffset(months=1)).dt.daysinmonth air_visit[\"daysToPrev25th\"] = air_visit.apply(lambda row:daysToPrev25th(row), axis=1) ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:2:1","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Visit Features Note that visitors does not exist in test set. Features such as locations and genres are categorical attributes and many missing values are involved. In this case, we need to create features on visitors , which will rely on the number of visitors to calculate. Therefore, we can feed visitors related features into our models to provide clues regarding stores. Here, we calculate min, max, median, mean, and the number of times each store has been visited per each day of the week. The following code might look complicated but we are using the aggregate method of “group by” essentially. def visitor_features(df, visitor_data): tmp = visitor_data.groupby(['air_store_id', 'dow'], as_index=False)['visitors'].min().rename(columns={'visitors': 'min_visitors'}) df = pd.merge(df, tmp, how='left', on=['air_store_id', 'dow']) visitor_data= train.copy() train = visitor_features(train, visitor_data) test = visitor_features(test, visitor_data) max visitors: maximum number of visitors for each store for a specific weekday min visitors: minimum number of visitors for each store for a specific weekday average visitors: average number of visitors for each store for a specific weekday count observations: number of observations for each store in a specific weekday. We take this feature into consideration because a number of stores don’t open to business everyday exponential moving average: the most recent data is weighted to have more of an impact ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:2:2","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Geographical Features Geographic features were not exactly straightforward to deal with. We had to do a bit of munging and pulling external data to determine the impact that locations had. City - We weren’t provided with columns such as region, city, neighorhood, etc. Instead, we had one column that included 5 levels of detail. Our dataset was in English, but the cities retained their original Japanese symbols. We had longitude and latitude data from 2 different systems to 6 decimal places, so the numbers sometimes did not match up exactly. So we need to split the column and select the information we need. Population and Density - To go along with this, we have also added the population for each of the cities. This information comes from both Simple Maps and verified with Wikipedia. This is the dataset that we combined with our training set. We made sure that every city was included and that there were no missing values. Population is in millions, population density is in thousands. cities = ['Tokyo-to','Osaka-fu','Fukuoka-ken','Hyogo-ken', 'Hokkaido','Hiroshima-ken','Shizuoka-ken', 'Miyagi-ken','Niigata-ken','Osaka','Kanagawa-ken', 'Saitama-ken'] population = [2.7, 1.2, 8.6, 1.1, 1.0, 1.2, 1.5,3.7, 1.5, .8, 2.6, 1.6] density = [11.9, 5.4, 13.9, 1.3, 1.3, 1.7, .5, 8.5,2.7, .7, 11.9, 4.4] pop = pd.DataFrame(list(zip(cities, population, density)), columns=['cities','population', 'density']) Restaurant Types - this information contains which food type the restaurant is selling. We are provided with restaurant types for every store in our data, but the labels are not consistent. In order to gain more meaningful data from the types of restaurants we are provided, we convert the many specific labels into more general labels that capture the restaurant type. After looking at the plot below, we are doubtful that restaurant type plays much of a role for the restaurants we are predicting. It doesn’t really seem like there is much of a relationship between restaurant type and expected visitors. # Quick example all_stores['Food_Type'] = np.where(all_stores['Food_Type'] .str.contains(('Izakaya'),'Izakaya',all_stores['Food_Type']) ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:2:3","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Data Cleaning Outliers As we can see, the number of visitors is highly left skewed and the max number reached 877 which seems unreasonable. These values might be outliers. We simply define outliers using the following rules: Data point that falls outside of 1.5 times of an interquartile range above the 3rd quartile and below the 1st quartile Data point that falls outside of 3 standard deviations. q1, q3= np.percentile(train.visitors,[25,75]) iqr = q3 - q1 lower_bound = q1 -(1.5 * iqr) upper_bound = q3 +(1.5 * iqr) train.loc[train.visitors \u003e upper_bound ,'visitors'] = upper_bound train.loc[train.visitors \u003c lower_bound ,'visitors'] = lower_bound After this process we will get a nicer distribution. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:2:4","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"3. Modeling Given the Time Series nature of our prediction exercise, we started by implementing the model ARIMA, short for Auto Regressive Integrated Moving Average, which is a model that explains a given time series based on its own past values, i.e. its own lags and the lagged forecast errors, so that equation can be used to forecast future values. But as we can see from this explanation, ARIMA model doesn’t take into consideration external factors to forecast the number of visitors. We then implemented another famous model for Time series which is Facebook Prophet, which is a procedure based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. However, this model works best with time series that have strong seasonal effects and several seasons of historical data, which is not the case in our prediction exercise. That’s when we shifted our approach to Light GBM, which is a gradient boosting framework that uses tree based learning algorithms. The advantage of this approach is that it doesn’t need any statistical assumptions in the backend. Besides, it determines which features have the highest discriminative powers, which enables us to give valuable insights to the restaurant owners. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:3:0","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"ARIMA # forecasting - register cores for parallel processing registerDoMC(detectCores()-1) fcst_matrix \u003c- foreach(i=1:nrow(train_ts),.combine=rbind, .packages=c(\"forecast\")) %dopar% { fcst_ets \u003c- forecast(ets(train_ts[i,]),h=fcst_intv)$mean fcst_nnet \u003c- forecast(nnetar(train_ts[i,]),h=fcst_intv)$mean fcst_arima \u003c- forecast(auto.arima(train_ts[i,]),h=fcst_intv)$mean fcst_ses \u003c- forecast(HoltWinters(train_ts[i,], beta=FALSE, gamma=FALSE),h=fcst_intv)$mean fcst_matrix \u003c- rbind(fcst_ets, fcst_nnet, fcst_arima, fcst_ses) } # post-processing the forecast table fcst_matrix_mix \u003c- aggregate(fcst_matrix,list(rep(1:(nrow(fcst_matrix)/4),each=4)),mean)[-1] fcst_matrix_mix[fcst_matrix_mix \u003c 0] \u003c- 0 colnames(fcst_matrix_mix) \u003c- as.character( seq(from = as.Date(\"2017-04-23\"), to = as.Date(\"2017-05-31\"), by = 'day')) fcst_df \u003c- as.data.frame(cbind(train_wide[, 1], fcst_matrix_mix)) colnames(fcst_df)[1] \u003c- \"air_store_id\" # melt the forecast data frame from wide to long format for final submission fcst_df_long \u003c- melt( fcst_df, id = 'air_store_id', variable.name = \"fcst_date\", value.name = 'visitors') fcst_df_long$air_store_id \u003c- as.character(fcst_df_long$air_store_id) fcst_df_long$fcst_date \u003c- as.Date(parse_date_time(fcst_df_long$fcst_date,'%y-%m-%d')) fcst_df_long$visitors \u003c- as.numeric(fcst_df_long$visitors) ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:3:1","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Prophet # Prediction from fbprophet import Prophet # This is used for suppressing prophet info messages. import logging logging.getLogger('fbprophet.forecaster').propagate = False number_of_stores = test['store_id'].nunique() date_range = pd.date_range(start=pd.to_datetime('2016-07-01'), end=pd.to_datetime('2017-04-22')) forecast_days = (pd.to_datetime('2017-05-31')- pd.to_datetime('2017-04-22')).days for cnt, store_id in enumerate(test['store_id'].unique()): print('Predicting %dof %d.'%(cnt, number_of_stores), end='\\r') data = visitor_data[visitor_data['air_store_id'] == store_id] data = data[['visit_date', 'visitors']].set_index('visit_date') # Ensure we have full range of dates. data = data.reindex(date_range).fillna(0).reset_index() data.columns = ['ds', 'y'] m = Prophet(holidays=df_holidays) m.fit(data) future = m.make_future_dataframe(forecast_days) forecast = m.predict(future) forecast = forecast[['ds', 'yhat']] forecast.columns = ['id', 'visitors'] forecast['id'] = forecast['id'].apply(lambda x:'%s_%s'%(store_id, x.strftime('%Y-%m-%d'))) forecast = forecast.set_index('id') test.update(forecast) print('\\n\\nDone.') ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:3:2","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"LightGBM #import lightgbm as lgbm from sklearn import metrics from sklearn import model_selection np.random.seed(42) model = lgbm.LGBMRegressor( objective='regression', max_depth=5, num_leaves=5 ** 2 - 1, learning_rate=0.007, n_estimators=30000, min_child_samples=80, subsample=0.8, colsample_bytree=1, reg_alpha=0, reg_lambda=0, random_state=np.random.randint(10e6) ) folds = 6 cv = model_selection.KFold(n_splits= folds, shuffle=True, random_state=42) val = [0] * folds sub = submission['id'].to_frame() sub['visitors'] = 0 feature_importances = pd.DataFrame(index=train_x.columns) for i, (fit_idx, val_idx) in enumerate(cv.split(train_x, train_y)): X_fit = train_x.iloc[fit_idx] y_fit = train_y.iloc[fit_idx] X_val = train_x.iloc[val_idx] y_val = train_y.iloc[val_idx] model.fit( X_fit, y_fit, eval_set=[(X_fit, y_fit), (X_val, y_val)], eval_names=('fit', 'val'), eval_metric='l2', early_stopping_rounds=200, feature_name=X_fit.columns.tolist(), verbose=False ) val[i] = np.sqrt(model.best_score_['val']['l2']) sub['visitors'] += model.predict(test_x, num_iteration=model.best_iteration_) feature_importances[i] = model.feature_importances_ print('Fold {} RMSLE: {:.5f}'.format(i+1, val_scores[i])) sub['visitors'] /= n_splits sub['visitors'] = np.expm1(sub['visitors']) val_mean = np.mean(val_scores) val_std = np.std(val_scores) print('Local RMSLE: {:.5f} (±{:.5f})'.format(val_mean, val_std)) ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:3:3","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"LSTM seq2seq + Encoder/Decoder Note that we have included all of the details below for this model in particular because this is our final model. Additionally, this model is best suited for this problem. Motivation LSTM is a great solution for relatively short sequences, up to 100-300 items. On longer sequences LSTM still works, but can gradually forget information from the oldest items. In our dataset, the timeseries is up to 478 days long, so we decided to implement encoder and decoder to “strengthen” LSTM memory. We are using the encoder and decoder structure in the Sequence-to-sequence learning (Seq2Seq) concept, which is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French). We used one LSTM RNN layer as “encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the “context”, or “conditioning”, of the decoder in the next step. And then we used another RNN layer with 2 hidden LSTM model acts as “decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called “teacher forcing” in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Data Preparation 1) Basic tidying of the training and test table # Drop unnecessary columns train_df = train_df.drop(columns=[ 'population', 'reserve_visitors', 'days_diff', 'day', 'season']) test = test.drop(columns=['population', 'reserve_visitors','days_diff', 'day', 'season']) # Refine column names train_df = train_df.rename({'visitors_x': 'visitors'}, axis = 1) train_df = train_df.rename({'day_of_week_y': 'day_of_week'}, axis = 1) train_df = train_df.rename({'month_y': 'month'}, axis = 1) train_df = train_df.rename({'longitude_y': 'longitude'}, axis = 1) train_df = train_df.rename({'latitude_y': 'latitude'}, axis = 1) test = test.rename({'latitude_y': 'latitude'}, axis = 1) test = test.rename({'longitude_y': 'longitude'}, axis = 1) test = test.rename({'month_y': 'month'}, axis = 1) test = test.rename({'day_of_week_y': 'day_of_week'}, axis = 1) # Clean unnecessary columns train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')] test = test.loc[:, ~test.columns.str.contains('^Unnamed')] # Fill the cells of missing values with -1 train_df = train_df.fillna(-1) test = test.fillna(-1) 2) Encode categorical columns There are several categorical columns in the dataset, which are ‘Food_Type’, ‘day_of_week’, ‘air_store_id’ that needs to be transferred. One-hot encoding may provide better result, but we applied labels encoding to avoid high dimensional feature space. # Weekday le_weekday = LabelEncoder() le_weekday.fit(train_df['day_of_week']) train_df['day_of_week'] = le_weekday.transform(train_df['day_of_week']) test['day_of_week'] = le_weekday.transform(test['day_of_week']) # id le_id = LabelEncoder() le_id.fit(train_df['air_store_id']) train_df['air_store_id'] = le_id.transform(train_df['air_store_id']) test['air_store_id'] = le_id.transform(test['air_store_id']) # food type le_ftype = LabelEncoder() le_ftype.fit(train_df['Food_Type']) train_df['Food_Type'] = le_ftype.transform(train_df['Food_Type']) test['Food_Type'] = le_ftype.transform(test['Food_Type']) 3) Simultaneous transformation of Train and test sets Considering the input data structure the LSTM RNN model needed, we filled up all the dates within the whole time span (2016-01-01 ~ 2017-05-31) for each stores with number of visitors as 0 on those dates, and the time-independent features (food types, longitude, latitude, etc) are “stretched” to timeseries leng","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:3:4","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"Evaluation Finally, we aggregated the prediction of both LGBM and seq2seq, which enabled us to get the best evaluation metric: Models Name ERROR(RMSLE) ARIMA. 0.56137 PROPHET 0.54208 LIHGT GBM(LGBM) 0.52412 LSTM SEQ2SEQ 0.50277 LSTM + LGBM 0.50002 The metric used to evaluate our model is RMSLE (root mean squared logarithmic error) $$ \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 } $$ where: n is the total number of observations p_i is the prediction of visitors a_i is the actual number of visitors We chose this metric for the following reasons: First, we care about percentage errors more than the absolute value of errors. Second, we want to penalize under estimating the number of visitors more than overestimating it, because underestimating will lead to a bad customer experience which is our top priority. ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:3:5","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"4. Business Insight Normally we would provide recommendations in this section. Because we are only looking to sell our system in this project, we are instead providing reasons for why someone should buy our product. We are looking to persuade a restaurant owner to purchase our product. For them, the decision ultimately comes down to: “How much will these predictions actually help me out? I believe I know the business and have good intuition about how many people will come to the restaurant. Are their predictions any better than me? How much money will they save for me?” To quantify this, we decided to run a couple of simulations on our training data based on intuition to see how good our results would be. For reference, our daily percent error (using MAPE) was 50%. Using the previous day, the manager would be off by 113% on average. Using the same weekday from the previous week, the manager would be off by 94% on average. # One day df['prev_day_visitors'] = df['visitors_x'].shift(1) df = df.groupby('air_store_id').apply(lambda group: group.iloc[1:, ]) # One week df['prev_week_visitors'] = df.groupby([df['visit_date'].dt.weekday])['visitors_x'].shift() df.groupby('air_store_id').apply(lambda group: group.iloc[7:, ]) # Error df['difference_decimal'] = abs( df['visitors_x'] - df['prev_day_visitors']) / df['visitors_x'] Staffing The hourly minimum wage in Japan translates to roughly $8.30 USD. Overstaffing by 2 people for a given 8-hour day equates to roughly $130 in unnecessary expenses. On the flip side, understaffing means a poor customer experience as wait time is longer. We believe this provides strong support for the purchase of our system. Supply Chain Although we are not able to quantify the supply chain as easily as we can with staffing costs, owners are able to reduce expenses if they have a more accurate picture of how many customers they expect to see. Purchasing too much leads to waste, and purchasing too little means running out of ingredients and making your customers upset. Using our system provides a more stable data-driven approach to this problem. Smoothing Demand Although not covered in our project, an individual will be able to look at their past data and gain an unbiased view of seasonality that occurs throughout the year. If they are looking for stability week-by-week or month-by-month to even out their supply purchases or keep the correct number of people on board, they can use their past data to aid in offering of incentives to drive customers where they see fit. Important Features Another insight we can provide the manager is an understanding of the most important features that drives their business. In particular, we found the following 5 to be most important when we ran our LightGBM model: (1) Previous year mapping (2) Days since previous 25th (3) Holiday flag (4) Aggregated visitors per day of week (5) Exponential Moving Average ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:4:0","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Machine learning"],"content":"5. Limitations \u0026 Next Steps To address the limitations of our predictive models, we could focus on the following dimensions to improve model performance. Feature Improvement We could pay more attention to feature engineering. We could include weather data in our model since the weather condition is likely to affect restaurant traffic. In addition, the distance of a restaurant to the busiest area might also contribute to the prediction of visiting frequency. Restaurants in busy areas are likely to have more traffic than those in suburb areas. We could also improve upon the feature selection process. The light GBM method provides the importance of each feature. Using this information, we are able to either remove unimportant features or add additional features that are similar to important features. Expand the Timespan Our current model is trained on data of 2-year time period. If we are able to collect data of longer time span and train our model on the enriched dataset, our model might be able to generate more accurate predictions. References https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html https://github.com/Arturus/kaggle-web-traffic https://github.com/MaxHalford/kaggle-recruit-restaurant/blob/master/Solution.ipynb https://www.kaggle.com/pureheart/1st-place-lgb-model-public-0-470-private-0-502 https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505 https://www.kaggle.com/h4211819/holiday-trick https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49100 ","date":"2019-11-12","objectID":"/2019-11-12-restaurants-visitors-predictive-model/:5:0","tags":["machine learning","Time Series Forecasting","regression"],"title":"Predicting Visitors for Restaurants","uri":"/2019-11-12-restaurants-visitors-predictive-model/"},{"categories":["Unsupervised learning"],"content":"Summary ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:1:0","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"The Business Problem Borussia Dortmund is one of the top teams in German Soccer League. But in some of the past seasons they have had inconsistent performances. Our objective is to help the team stay consistent with their performance and win the German League. Our analysis is directed to help the coach of Borussia Dortmund take strategic decisions and make sure that team becomes the champion in the next season. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:1:1","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"What is Success? We define success as winning more games and getting as many points as possible. Soccer is a highly competitive game and a good team is the team who always wins the game. Winning more games means more reputation, more fans, more money. The nature of competitive sports means that the only definition for success is win. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:1:2","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Approach Codes in detail can be found here: github repo A team generally has two options to maximize wins - either it can hire better players or it can improve the game play strategy. Since we do not have data regarding Dortmund’s budget or player costs, we will focus on the latter. Good coaching is essential to a soccer team and the best coach can earn more than $10 million a year. Generally, a team’s coach has different plans for different games. For example, if Dortmund is going to play against a weak team in the league, the best way is to keep the ball under control and keep attacking them. However, when Dortmund is going to play against one of the best team in the history, FC Bayern Munich, who is in the same league, Dortmund needs to focus on defense and hope the counterattack would work. We want to analyze what sets of strategies work best depending on who we are playing agaist. We hope to make a guidance manual for Dortmund’s coach to help him choose the best game strategy and improve the winning rate. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:1:3","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Data Preparation To start our analysis, we fetched match data related only to Dortmund. We dropped all irrelevant columns for our analysis. Adding flag for home/away matches: A team always has 2 legs against any opponent - home and away matches. Generally a team’s performance varies a lot in home and away matches due to the change in the environment and playing atmosphere. So we split the dataset into these 2 categories to see if there is any discernible patterns between their performances at home and away games. dortmund_matches$home_or_away \u003c- ifelse(dortmund_matches$home_team_api_id == 9789,'home', 'away') Creating a column for opponent team strength: We want to classify opoonent teams as strong, equal and weak on a match by match basis by their overall team rating. strong_opponent - Overall rating of opponent team \u003e 2.5 than Dortmund equal_opponent - Overall rating of opponent team is in between +2.5 to -2.5 of Dortmund weak - Overall rating of opponent team \u003c 2.5 than Dortmund We choose these intervals as Fifa generally gives ratings to the players such that teams’ main playing XI have relatively similar scores. For example, the overall team rating for 2 of the top clubs in the world, Barcelona and Real Madrid, have an overall team rating of 344 and 343 respectively. So we give a buffer of 2.5 points for classifying teams as same ‘level’. Source:fifaindex Below we categorize our possession information as more, less or same. more_possession - when our ball possession is more than 55% less_possession - when our ball possession is less than 45% same_possession - when our ball possession is in between 45% to 55% ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:2:0","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Exploratory Data Analysis ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:3:0","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Ranking We start by looking at Dortmund’s ranking over the past couple of years. Table below shows that Dortmund is one of the top teams in the German league. Actually, if we look at the table below, we can conclude that Dortmund is considered the second best team in German League. In order to win the championship (get rank 1) within German league, a team needs to score the highest number of points. Points are allotted to a team based on match outcomes - a win gives 3 points, tie gives 1 point, while lost match has 0 points. In case of a tie in the number of points, Goal Difference is considered to decide rank. Goal Difference is the difference between Goals scored and Goals conceded. Season Rank Club Points Goal Difference 2015-16 1 Bayern 88 63 2 Dortmund 78 48 3 Bayer 60 16 4 Monchengladbach 55 17 5 Schalke 04 52 2 6 Mainz 50 4 7 Hertha 50 0 2014-15 1 Bayern 79 62 2 Wolfsburg 69 34 3 Monchengladbach 66 27 4 Bayer 61 25 5 Augsburg 49 0 6 Schalke 04 48 2 7 Dortmund 46 5 2013-14 1 Bayern 71 90 2 Dortmund 71 42 3 Schalke 04 64 20 4 Bayer 61 19 5 Wolfsburg 60 13 6 Monchengladbach 55 16 7 Mainz 53 -2 2012-13 1 Bayern 91 80 2 Dortmund 66 39 3 Bayer 65 26 4 Schalke 04 55 8 5 Freiburg 51 5 6 Eintracht 51 3 7 Hamburg 48 -11 2011-12 1 Dortmund 81 55 2 Bayern 73 55 3 Schalke 04 64 30 4 Monchengladbach 60 25 5 Bayer 54 34 6 VfB Stuttgart 53 17 7 Hannover 48 -4 2010-11 1 Dortmund 75 45 2 Bayer 68 20 3 Bayern 65 41 4 Hannover 60 4 5 Mainz 58 12 6 Nurnberg 47 3 7 Kaiserslautern 46 -3 2009-10 1 Bayern 70 41 2 Schalke 04 65 22 3 Werder Bremen 61 31 4 Bayer 59 21 5 Dortmund 57 12 6 VfB Stuttgart 55 10 7 Hamburg 52 15 Rank Source: https://www.espn.com/soccer/standings/_/league/ger.1/season/2009/german-bundesliga Interpretation We observe that although Dortmund is able to maintain rank in top 2 in most years, they have not won the rank 1 since past 4 years. To get rank 1 and win the championship, Dortmund would have to score points higher than FC Bayern (which has been the champion in most seasons). The difference in points in the last 2 seasons has been of 10 points. So winning about even 4 more games would help Dortmund in winning the German league championship or at least end up in the top 3. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:3:1","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Goals To win more games, Dortmund either needs to score more goals (have a good attack) or concede less goals (have a good defence). Let’s have a look at their goals statistics: Historically, all German teams have been improving, by both scoring more goals and conceding fewer goals than previous seasons. Over time, Dortmund has improved their goal scoring capabilities from ~1.75 to ~2.4, but their defence seems to be fairly constant with an average of about 1. The graph above shows that conceded goals have increased. On the other hand, their most popular rival, Bayern Munich significantly improved their defense while maintaining strong attacking capabilities. Bayern’s average goals scored only improved marginally from 2.19 to 2.35, but the average number of goals conceded has reduced significantly from 1.24 to 0.5. A quick look at Dortmund’s past strategy tells us that this constant goals concede might be because of no change in their defense strategy since 5 seasons. test \u003c- team_atts %\u003e% filter(team_api_id == 9789) %\u003e% subset(select = c(year, defencePressureClass, defenceAggressionClass, defenceDefenderLineClass)) test {\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"year\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"defencePressureClass\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"defenceAggressionClass\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"defenceDefenderLineClass\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"2010\",\"2\":\"Medium\",\"3\":\"Double\",\"4\":\"Offside Trap\",\"_rn_\":\"1\"},{\"1\":\"2011\",\"2\":\"Medium\",\"3\":\"Double\",\"4\":\"Cover\",\"_rn_\":\"2\"},{\"1\":\"2012\",\"2\":\"Medium\",\"3\":\"Double\",\"4\":\"Cover\",\"_rn_\":\"3\"},{\"1\":\"2013\",\"2\":\"Medium\",\"3\":\"Double\",\"4\":\"Cover\",\"_rn_\":\"4\"},{\"1\":\"2014\",\"2\":\"Medium\",\"3\":\"Double\",\"4\":\"Cover\",\"_rn_\":\"5\"},{\"1\":\"2015\",\"2\":\"Medium\",\"3\":\"Double\",\"4\":\"Cover\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}} Interpretation: This is a clear indication that in order to improve their defense performance, Dortmund should try different strategies against different set of opponents to conceded minimum goals possible. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:3:2","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Overall Matches Performance Now we will deep dive to understand how is Dortmund’s overall match performance with respect to its different types of opponents (strong, equal and weak). Interpretation: From the above plot we see that Dortmund’s performance varies depending on level of opponent and whether they are playing at home or as an away team. Over 8 years, Dortmund played 272 games in total. Since Dortmund is a strong team, in most of the matches (192 matches) the opponent team is categorized as weak_opponent. In 55 matches, their opponent team is categorized as equal_opponent and in 25 matches Dortmund played against strong_opponent. We can observe two things in the plot - first, as expected, Dortmund has the highest winning rate agaist weak_opponent, followed by equal_opponent, and lastly strong_opponent. Among 192 games against ‘weak’ teams, Dortmund won 124 (65% winning rate). However, Dortmund only won 10 out of 25 games(40% winning rate) when playing against ‘strong team’. Second, Dortmund’s performance is better when they play home game compared to away games. For instance, they have a 73% winning rate when played against weak_opponent at home and only 56% winning rate when playing in away matches. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:3:3","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Conclusions Since Dortmund’s performance depends on their opponents’ level and whether they are playing at home or not, coach should adjust the team’s strategy according to these circumstances. So, we seperate Dortmund’s matches into six categories: home games -\u003e against weak opponent away games -\u003e against weak opponent home games -\u003e against equal opponent away games -\u003e against equal opponent home games -\u003e against strong opponent away games -\u003e against strong opponent Then we analyze what strategy works best in each category by using association rules. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:3:4","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Association Rules ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:4:0","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Effect of Ball Possession on match outcome We will first analyze ball possession and its impact on match outcome. Explanation Possession is a crucial factor in a soccer game. Possession means the percentage of time a team has a ball under its control. For instance, if Dortmund has a possession of 60 in a game, it means 60% of time the ball is under the control of Dortmund’s players. Possession can reflect the strategy of a team. If a team has a high possession in a game, it means the team’s strategy is to keep attacking opponent. However, having the ball under control means the player will lose more energy and the defense will be weak since most players need to participate in offense part and ignore the defense task. On the other end, giving up the ball control means a team is trying to save the energy and utilize the counterattack chance to destroy the opponent at once. In short, possession reflects a team’s basic strategy in a game. We categorize our possession value so that if we have more than 55% possession, then we conclude we have more possession than the opponent. If our possession is between 45% and 55%, we conclude that both teams have equal possession. If our possession is under 45%, we conclude that our possession is less than opponent. We have calculated our winning rates according to above mentioned 6 categories i.e. if we are playing against strong/equal/weak team at home/away. We will analyze each category individually. For example, our winning rate for weak team/home game is 0.73, we want to see the rule with confidence over 0.73 with ‘win’ at the right hand side because this can show us only those useful strategy which can improve our performance. ## away equal_opponent home less_possession ## 75 37 77 59 ## lose more_possession same_possession strong_opponent ## 33 69 24 17 ## tie weak_opponent win ## 33 98 86 {\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"V1\"],\"name\":[1],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"V2\"],\"name\":[2],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"V3\"],\"name\":[3],\"type\":[\"fctr\"],\"align\":[\"left\"]},{\"label\":[\"V4\"],\"name\":[4],\"type\":[\"fctr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"home\",\"2\":\"tie\",\"3\":\"weak_opponent\",\"4\":\"more_possession\",\"_rn_\":\"1\"},{\"1\":\"away\",\"2\":\"lose\",\"3\":\"equal_opponent\",\"4\":\"less_possession\",\"_rn_\":\"2\"},{\"1\":\"away\",\"2\":\"win\",\"3\":\"weak_opponent\",\"4\":\"less_possession\",\"_rn_\":\"3\"},{\"1\":\"home\",\"2\":\"tie\",\"3\":\"strong_opponent\",\"4\":\"same_possession\",\"_rn_\":\"4\"},{\"1\":\"away\",\"2\":\"lose\",\"3\":\"strong_opponent\",\"4\":\"same_possession\",\"_rn_\":\"5\"},{\"1\":\"home\",\"2\":\"tie\",\"3\":\"strong_opponent\",\"4\":\"same_possession\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}} Analyzing home games against weak_opponents Since the winning rate right now is 0.73, we want to generate rules that give us a winning rate greater than 0.73. From the result, we can see that if we keep our possession the same as our opponent, we can actually increase our winning chances by 43%. rules \u003c- apriori(soccer, parameter = list(supp = 0.03, conf = 0.1)) rules \u003c- sort(rules, by = \"confidence\", decreasing = TRUE) rules %\u003e% subset(subset = (rhs %pin% \"win\")) %\u003e% subset(subset = confidence \u003e 0.72) %\u003e% subset(subset = (lhs %pin% 'home' \u0026 lhs %pin% 'weak_opponent')) %\u003e% inspect() ## lhs rhs support confidence ## [1] {home,less_possession,weak_opponent} =\u003e {win} 0.08552632 0.8125 ## lift count ## [1] 1.436047 13 Analyzing away games against ‘weak_opponents’ We can see that right now our winning rate is 0.56. If we play with same possession, we will improve our winning chances by 6%. However, the count is only 3 in this case. ## lhs rhs support confidence ## [1] {away,same_possession,weak_opponent} =\u003e {win} 0.01973684 0.6 ## lift count ## [1] 1.060465 3 Analyzing home games against ‘strong_opponents’ We can see that right now our winning rate is 0.42. From our dataset we can see that if we play with less possesion, our winning rate can go u","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:4:1","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Effect of Team Attributes on match outcome Next we analyze team attributes and their impact on match outcome. Explanation Now we will use team attributes data to generate rules to develop strategies for our team’s coach. Team attribution table contains the strategy which a team uses and it is updated every season. We combined eight features into every game for both Dortmund and opponent’s team. Some of the features are buildup passing class, chance crossing class, and defense pressure class. For example, if the passing class is ‘long’, it means Dortmund is focusing on long passing in the game. If the passing class is ‘mixed’, it means Dortmund will use both short passes and long passes in a game. If chance creation crossing class is ‘normal’, it means Dortmund is going to play normally. But when chance creation crossing class is ‘risky’, it means Dortmund will try to make some risky crosses even though it might lead to lost of possession or even conceding a goal. We will explain what each term mean in the following section. Terms 1. Play Speed Slow: Team plays a slow pace game Balanced: Team plays with a balanced pace game Fast: Team plays with a fast pace game 2. Play Passing Short: Team focuses on short passing Mixed: Team does both short passing and long passing Long: Team focuses on long passing 3. Chance creation passing Safe: Teams plays safe when there is a chance Normal: Team plays normally when there is a chance Risky: Team plays with risks when there is a chance 4. Chance creation crossing Lots: Team tends to try lots of cross passings Little: Team tends to try little cross passings 5. Chance creation positioning Organised: Team asks players to play by plan Free Form: Team allows player to play with improvise 6. Defense Pressure Deep: Team focuses less on defense Medium: Team is average on defense High: Team focuses a lot on defense 7. Defense Aggression Contain: Team is conservative and does not want to commit fouls on defense Press: Team give pressure on defense Double: Team will double the offensive player on defense 8. Defender line class Cover: Team plays with normal defense strategy Offside Trap: Team sets up off side traps on defense We added ‘Dortmund’ and ‘Opponent’ before each term to distinguish between them. Approaches We have 19 columns in our table and we want our rule as specific as possible, so that we need to adjust parameter accordingly for each of six analysis. For example, when we analyze playing against weak team at home, if we set our minimum length to 16, we will generate 8341 rules. That many rules are unnecessary. If we set our minimum length to 17, we will have 826 rules this time. After filtering the condition, we will have a total of 16 applicable rules, which are adequate. Also, we need to adjust our support each time. Because Dortmund plays a lot of home game against weak team, we can set up our support high for that category. However, since Dortmund only plays a few home games against equal team (12 games), we need to lower our support. Also, we need to filter by confidence each time to make sure we have only those rules that can increase our winning rate. Analyzing home games against ‘weak_opponents’ for win Since the winning rate right now is 0.73, we want to generate rules that gives us a winning rate greater than 0.74. rules1 \u003c- apriori(team,parameter = list(supp = 0.03, conf = 0.33, maxlen = 20, minlen = 7)) rules1 \u003c- sort(rules1,by = \"confidence\",decreasing = TRUE) rules1 \u003c- rules1 %\u003e% subset(subset = (rhs %pin% \"win\")) %\u003e% subset(subset = (lhs %pin% 'home' \u0026 lhs %pin% 'weak_opponent')) %\u003e% subset(subset = confidence \u003e 0.73) inspect(rules1[1:3]) ## lhs rhs support confidence lift count ## [1] {Dortmund_Normal_Shooting, ## Dortmund_Risky_Passing, ## home, ## Opponent_Normal_Passing, ## Opponent_Normal_Shooting, ## weak_opponent} =\u003e {win} 0.06862745 0.9333333 1.511111 14 ## [2] {Dortmund_Free Form_Positioning, ## Dortmund_Normal_Shooting, ## home, ## Opponent_Normal_Passing, ## Oppo","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:4:2","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"TL;DR / Conclusion ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:5:0","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Findings From our analysis above, based on team attribution table, we can conclude that this is a good approach to generate rules for each categories. However, since we do not have a large sample size for games agaist equal teams and strong teams, the rules work the best when Dortmund plays against weak teams because we have enough counts to generate a detailed plan. The association rules provide us with insights for the strategies that are effective and not so effective for Dortmund against weak teams in both home and away games. Dortmund generally performs very well at home which is well known. The results slightly change when they play away games where their wins reduce and the number of ties and losses go up significantly as well. These are areas that elude them of vital points that give them a shot at the top 3 spots of the league. One prominent feature resulting most of their games which end up in ties and losses in both home and away games are that even though they take a lot of shots, they fail to net the ball into the goal. It could be due to the extra pressure in the away conditions and poor finishing. This is a huge area for improvement. If Dortmund can convert even one of the many shots that they have at the goal, it could be the differentiating factor in converting a loss to a tie and changing a tie to a win. The most import finding from our analysis is that strategies do impact game outcomes in a big way. When Dortmund plays with correct strategies, they can increase their winning rates significantly. By studying the ratings and features of opponents, Dortmund can adjust its plan accordingly and this will provide team with an advantage and a higher chance to win. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:5:1","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Recommendations We believe that the only way for a club to get the top spot in the league is to score as many points as possible either through wins or ties. Given the fact that we have a dominant team FC Bayern Munich who always has much more budget than Dortmund and can easily sign the best players, it is difficult to beat them by forming a team with better skills than FC Bayern Munich. Instead, the best approach to achieve success is to pick the best strategy each game especially against weaker opponents since that is where we can get most points. An area for significant improvement is the shot accuracy and finishing. Dortmund should give more intense drills and practice on finishing touches and shots. Our analysis has shown the huge potentianl benefits that can be brought by correct strategies. Based on our analysis, we suggest that Dortmund should look into both Dortmund’s and opponent’s team status such as possession, defense tasks, passing styles and other features and find how each feature can affect the game outcome. In short term, we suggest that Dortmund should start to apply the rules we found through our analysis immediately. These rules have no cost or minimal costs to implement. We believe the coach of one of the best teams in the German League would already know how to employ various strategies and we wanted to show what are the factors and strategies that lead to Dortmund’s wins and losses. This way, we provide rules or effective strategies that would work against various types of oppositions and the coach would have to effectively communicate these tactics to the players and employ player formations suitable for such strategies. We are very confident that our rules toward weak teams will be effective. Due to the low counts and small sample size of our rules toward equal team and strong teams, we hope that coach can investigate and verify them before applying. In long term, we suggest that Dortmund should establish a database which focuses on collecting information on match-level and team-level data for all its opponents. With more information, Dortmund can construct a more precise model that can generate detailed suggestions for team’s coach before each game. Buying good players can boost Dortmund’s performance for one or two years but establishing a database and constructing a data-driven model will benefit the club in the ong run. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:5:2","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Unsupervised learning"],"content":"Limitations We have specific strategies that should be executed for different scenarios. However, we are not sure whether the team can successfuly apply them. For instance, for some games rules suggest that Dortmund will have a higher winning chance by doubling the defense. Coach might not be able to apply this if the team does not have enough defensive players at the moment. We need to keep in touch with the coach in order to adjust strategies based on current status. Some confidence values from our rules can be inaccurate because of the lack of a big sample size. For example, when we analyze rules against strong teams, a certain strategy gives us a confidence value of 1. This means if we apply this strategy, our winning rate can be 100%. This cannot be true since the count is only 3, which means the winning rate is overestimated. Even though these rules are useful, we need to be cautious and it is difficult to know what the real confidence value is. Setting hyper-parameters for association rules can be tricky. If we want to have a very specific rule, we need to set our minimum length high, and this will lead to less rules. However, if we set our minimum length low, we will have some rules with really high confidence, lift but smaller length. It is up to coach team’s choice what kind of rules they would like to have. As a future consideration, we will combine our suggestions with real life outcomes to find out the best hyper-parameters for our association rules. ","date":"2019-11-02","objectID":"/2019-11-02-money-ball-with-arules/:5:3","tags":["association rules","unsupervised learning"],"title":"Moneyball on Dortmund with Association Rules","uri":"/2019-11-02-money-ball-with-arules/"},{"categories":["Machine learning"],"content":"In this post, I will apply decision tree, k-NN, SVM to predict the evaluation of the cars based on their characteristics. Dataset is from UCI ML Repository. More specifically, I will explore how well these techniques perform for several different parameter values. Present a brief overview of the predictive modeling process, explorations, and discuss my results. Then I will present the final model and discuss its performance in a comprehensive manner (overall accuracy; per-class performance, i.e., whether this model predicts all classes equally well, or if there some classes for which it does much better than others; etc.) Let’s hit the road. ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:0:0","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Data Exploration #Importing the basic libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.style.use('ggplot') import plotly.offline as py import seaborn as sns %matplotlib inline #%config InlineBackend.figure_format = 'svg' from warnings import simplefilter # ignore all future warnings simplefilter(action='ignore', category=FutureWarning) cars = pd.read_csv('car.data', names = ['buying', 'maint', 'doors','capacity','lug_boot','safety','class']) #Taking an overview of data cars.sample(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } buying maint doors capacity lug_boot safety class 975 med high 2 2 med low unacc 277 vhigh med 4 2 big med unacc 1219 med low 3 2 med med unacc 1316 low vhigh 2 more small high unacc 249 vhigh med 3 2 big low unacc 1712 low low 5more 4 small high good 1676 low low 4 2 small high unacc 1374 low vhigh 4 more big low unacc 1117 med med 3 4 small med acc 541 high high 2 2 small med unacc cars.doors.replace(('5more'),('5'),inplace=True) cars.capacity.replace(('more'),('5'),inplace=True) cars.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } buying maint doors capacity lug_boot safety class count 1728 1728 1728 1728 1728 1728 1728 unique 4 4 4 3 3 3 4 top med med 2 2 med med unacc freq 432 432 432 576 576 576 1210 The count for every feature is the same as the number of rows, which indicates no missing values. Yay! Since we are dealing with categorical data, we are shown the distinct values in the unique column. The distribution of the acceptability of the cars. #Lets find out the number of cars in each evaluation category cars['class'].value_counts() unacc 1210 acc 384 good 69 vgood 65 Name: class, dtype: int64 sns.countplot(cars['class']) As we can see, our target varible is highly skewed ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:1:0","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Initial Feature Exploration So we need to predict the acceptability of the car given the 6 features. Let’s try to find the relationship between each feature variable with the target variable. I’ll use pandas crosstab to make a table showing the relationship and Plotly to plot an interactive graph for the same. buy = pd.crosstab(cars['buying'], cars['class']) maint = pd.crosstab(cars['maint'], cars['class']) drs = pd.crosstab(cars['doors'], cars['class']) prsn = pd.crosstab(cars['capacity'], cars['class']) lb = pd.crosstab(cars['lug_boot'], cars['class']) sfty = pd.crosstab(cars['safety'], cars['class']) buy .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } class acc good unacc vgood buying high 108 0 324 0 low 89 46 258 39 med 115 23 268 26 vhigh 72 0 360 0 buy.plot.bar(stacked=True) maint.plot.bar(stacked=True) drs.plot(kind='bar',stacked=True) sfty.plot.bar(stacked=True) ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:2:0","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Encoding and Data Spliting We need to encode the categorical data There are two options, either we use label encoder or one hot encoder. Intuitively, predictors’ value in the dataset such as ‘low, med, high’ introduce an underlying linear order itself, therefore, it’s alright to transform data with ordinal encoder. cars1 = cars.copy() cars1['class'].replace(('unacc', 'acc', 'good', 'vgood'), (1, 2, 3,4), inplace = True) cars1['buying'].replace(('vhigh', 'high', 'med', 'low'), (4,3, 2, 1), inplace = True) cars1['maint'].replace(('vhigh', 'high', 'med', 'low'), (4,3, 2, 1), inplace = True) cars1['lug_boot'].replace(('small','med','big'),(1,2,3),inplace=True) cars1['safety'].replace(('low','med','high'),(1,2,3),inplace=True) print(\"Feature Correlation:\\n\") fig, ax = plt.subplots(figsize=(9,7)) ax.set_ylim(6.0, 0) ax=sns.heatmap(cars1.corr(),center=0,vmax=.3,cmap=\"YlGnBu\", square=True, linewidths=.5, annot=True) Feature Correlation: Ignoring the diagonal values, it can be seen that most of the columns shows very weak correlation with ‘class’. ‘safety’ column is having a correlation with ‘class’. #Dividing the dataframe into x features and y target variable X1 = cars1.drop(['class'],axis = 1) y1 = cars1['class'] from sklearn.model_selection import train_test_split X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=42) X3 = cars.drop(['class'],axis = 1) y3 = y1 # Using pandas dummies function to encode categorical data X3 = pd.get_dummies(X3,columns= ['buying','capacity','doors','maint','lug_boot'], prefix_sep='_', drop_first=True) X3['safety'].replace(('low','med','high'),(0,1,2),inplace=True) X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = 0.3, random_state = 41) ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:3:0","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Model Building ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:4:0","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"KNN from __future__ import print_function from sklearn.metrics import classification_report from sklearn.model_selection import GridSearchCV from sklearn.neighbors import KNeighborsClassifier #create a dictionary of all values we want to test for n_neighbors param_grid = {'n_neighbors': np.arange(1,15)} scores = ['precision', 'recall'] for score in scores: print(\"# Tuning hyper-parameters for %s\" % score) print() knn_gscv = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='%s_micro' % score) knn_gscv.fit(X1_train, y1_train) print(\"Best parameters set found on development set:\\n\") print(knn_gscv.best_params_) print(\"\\nGrid scores on development set:\\n\") means = knn_gscv.cv_results_['mean_test_score'] stds = knn_gscv.cv_results_['std_test_score'] print(\"\\nDetailed classification report:\") print(\"\\nThe model is trained on the full development set.\") print(\"\\nThe scores are computed on the full evaluation set.\\n\") y_true, y_pred = y1_test, knn_gscv.predict(X1_test) print(classification_report(y_true, y_pred)) precision recall f1-score support 1 0.97 0.99 0.98 358 2 0.90 0.88 0.89 118 3 0.78 0.74 0.76 19 4 0.89 0.67 0.76 24 accuracy 0.94 519 macro avg 0.88 0.82 0.85 519 weighted avg 0.94 0.94 0.94 519 # Plot K vs accuracy avg_score=[] for k in range(2,15): knn=KNeighborsClassifier(n_neighbors=k) score=cross_val_score(knn,X1_train,y1_train,cv=5,scoring='accuracy') avg_score.append(score.mean()) plt.figure(figsize=(8,5)) plt.plot(range(2,15),avg_score) plt.xlabel(\"n_neighbours\") plt.ylabel(\"accuracy\") plt.title(\"K value vs Accuracy Plot\") Both grid search cross validation and plot show that neighbor = 5 is a potential good hyperparameter. #Using KNN classifier, knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) knn.fit(X1_train, y1_train) y1_pred = knn.predict(X1_test) f1_KNN = f1_score(y1_test,y1_pred, average='micro') print(\"Training Accuracy: \",knn.score(X1_train, y1_train)) print(\"Testing Accuracy: \", knn.score(X1_test, y1_test)) print(\"Cross-Validation Score :{0:.3f}\".format(np.mean(cross_val_score(knn, X1, y1, cv=5)))) Training Accuracy: 0.9818031430934657 Testing Accuracy: 0.9421965317919075 Cross-Validation Score :0.813 ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:4:1","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"SVM Grid Search from sklearn.metrics import classification_report from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC # Set the parameters by cross-validation parameters = [{'kernel': ['rbf'], 'gamma': 10. ** np.arange(-5, 4), 'C': [0.1, 1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}] scores = ['precision', 'recall'] for score in scores: print(\"# Tuning hyper-parameters for %s\" % score) print() svc_gscv = GridSearchCV(SVC(), parameters, cv=5, scoring='%s_micro' % score) svc_gscv.fit(X1_train, y1_train) print(\"Best parameters set found on development set:\\n\") print(svc_gscv.best_params_) print(\"\\nGrid scores on development set:\\n\") means = svc_gscv.cv_results_['mean_test_score'] stds = svc_gscv.cv_results_['std_test_score'] print(\"\\nDetailed classification report:\") print(\"\\nThe model is trained on the full development set.\") print(\"\\nThe scores are computed on the full evaluation set.\\n\") y_true, y_pred = y1_test, svc_gscv.predict(X1_test) print(classification_report(y_true, y_pred)) # Tuning hyper-parameters for precision Best parameters set found on development set: {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'} Grid scores on development set: Detailed classification report: The model is trained on the full development set. The scores are computed on the full evaluation set. precision recall f1-score support 1 0.99 0.99 0.99 358 2 0.96 0.95 0.95 118 3 0.85 0.89 0.87 19 4 0.92 0.92 0.92 24 accuracy 0.97 519 macro avg 0.93 0.94 0.93 519 weighted avg 0.98 0.97 0.98 519 # Tuning hyper-parameters for recall Best parameters set found on development set: {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'} Grid scores on development set: Detailed classification report: precision recall f1-score support 1 0.99 0.99 0.99 358 2 0.96 0.95 0.95 118 3 0.85 0.89 0.87 19 4 0.92 0.92 0.92 24 accuracy 0.97 519 macro avg 0.93 0.94 0.93 519 weighted avg 0.98 0.97 0.98 519 ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:4:2","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Fit SVC rbf From the GridSearch result, we find that with kernel = 'rbf', C = 100, gamma = 0.1, the model can achive best performance with respect to recall and accuracy. Since the unbalanced label of our target, I decide to go with recall, intuitively because we want to capture as many cars that will not be accepted as possible. from sklearn.svm import SVC svc_rbf = SVC(kernel = 'rbf', C = 100, gamma = 0.1) svc_rbf.fit(X1_train,y1_train) y1_pred = svc_rbf.predict(X1_test) f1_SVC_rbf = f1_score(y1_test,y1_pred, average='micro') print(\"Training Accuracy: \",svc_rbf.score(X1_train, y1_train)) print(\"Testing Accuracy: \", svc_rbf.score(X1_test, y1_test)) print(\"Cross-Validation Score :{0:.3f}\".format(np.mean(cross_val_score(svc_rbf, X1, y1, cv=5)))) Training Accuracy: 0.9983457402812241 Testing Accuracy: 0.9749518304431599 Cross-Validation Score :0.877 Learning curve from sklearn.model_selection import learning_curve from sklearn.svm import SVC plt.figure() plt.xlabel(\"Training examples\") plt.ylabel(\"Score\") train_sizes, train_scores, test_scores = learning_curve( svc_rbf, X1_train, y1_train, cv=5, n_jobs=1) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid() plt.title(\"Learning Curves (SVM, RBF kernel,C=100, $\\gamma=0.1$)\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\") plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\") plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\") plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\") plt.legend(loc=\"best\") plt.show() ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:4:3","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Decision Tree ind Hyperparameter # Plot max_depth vs accuracy from sklearn.tree import DecisionTreeClassifier avg_train=[] avg_test=[] for max_depth in range(2,11): dtree = DecisionTreeClassifier(max_depth=max_depth) train_score=cross_val_score(dtree,X1_train,y1_train,cv=5,scoring='accuracy') test_score =cross_val_score(dtree,X1_test,y1_test,cv=5,scoring='accuracy') avg_train.append(train_score.mean()) avg_test.append(test_score.mean()) plt.figure(figsize=(8,5)) plt.plot(range(2,11), avg_train,color=\"r\",label=\"Training score\") plt.plot(range(2,11), avg_test, color=\"g\", label=\"Test score\") plt.legend() plt.xlabel(\"max_depth\") plt.ylabel(\"accuracy\") Max depth of 9 looks to be a balanced cutoff point Fit the model #Trying decision tree classifier dtree = DecisionTreeClassifier(random_state = 0, max_depth=9) dtree.fit(X1_train, y1_train) y1_pred = dtree.predict(X1_test) F1_dtree = f1_score(y1_test,y1_pred, average='micro') print(\"Training Accuracy: \",dtree.score(X1_train, y1_train)) print(\"Testing Accuracy: \", dtree.score(X1_test, y1_test)) cm = confusion_matrix(y1_test, y1_pred) print('\\n',cm,'\\n') Training Accuracy: 0.9842845326716294 Testing Accuracy: 0.9556840077071291 ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:4:4","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Random Forest Baseline Model from sklearn.ensemble import RandomForestClassifier rfc=RandomForestClassifier(random_state=51) rfc.fit(X1_train,y1_train) y1_pred = rfc.predict(X1_test) print(\"Training Accuracy: \",rfc.score(X1_train, y1_train)) print(\"Testing Accuracy: \", rfc.score(X1_test, y1_test)) Training Accuracy: 1.0 Testing Accuracy: 0.9402697495183044 So, the basic model of RFC is giving 94% accuracy, but training score is clearly overfit. Now, check the effect of n_estimators on the model Fine Tune Hyperparameter # Plot number of trees vs accuracy n_tree=[10,25,50,100] curve = validation_curve(rfc,X1_train,y1_train,cv=5,param_name='n_estimators', param_range=n_tree) train_score=[curve[0][i].mean() for i in range (0,len(n_tree))] test_score=[curve[1][i].mean() for i in range (0,len(n_tree))] f,ax=plt.subplots(1) plt.plot(n_tree,train_score) plt.plot(n_tree,test_score) plt.xticks=n_tree plt.xlabel(\"n_estimators\") plt.ylabel(\"accuracy\") plt.title(\"number of trees vs Accuracy Plot\") \u003cimg src=’/images/car_value/output_71_1.png’ width=\"200”) So, with the increasing n_estimators, test accuracy is increasing. Model is evaluating best at n_estimators=50. After n_estimators = 50, model starts overfitting. Now, we’ve reached approx. 97.1% accuracy. Now, check how the model fits for various values of ‘max_features’ rfc = RandomForestClassifier(n_estimators=50,random_state=51) rfc.fit(X1_train,y1_train) param_range=range(1,len(X1.columns)+1) curve=validation_curve(RandomForestClassifier(n_estimators=50,random_state=51),X1_train,y1_train,cv=5, param_name='max_features',param_range=param_range) train_score=[curve[0][i].mean() for i in range (0,len(param_range))] test_score=[curve[1][i].mean() for i in range (0,len(param_range))] f, ax = plt.subplots(1,figsize=(5,5)) plt.plot(param_range,train_score, label='training') plt.plot(param_range,test_score,label='test') plt.xticks=param_range plt.legend() plt.title('validation_curve of random forest with 50 trees') Deal with overfitting From above graph, it is clear that model is giving best resut for max_features=5. Still the model is overfitting. Now we’ve reached 97.2% accuracy approx. We can also check of other parameters like ‘max_depth’,‘criterion’,etc using above code.Another simple way is to use GridSearch to get combination of best parameters. As this dataset is small, GridSearch will take less time to complete. param_grid={'criterion':['gini','entropy'], 'max_depth':[2,5,10,20], 'max_features':[2,4,5,6,'auto'], 'max_leaf_nodes':[2,3,None],} grid=GridSearchCV(estimator=RandomForestClassifier(n_estimators=50,random_state=51), param_grid=param_grid,cv=10) grid.fit(X1_train,y1_train) print(grid.best_params_) print(grid.best_score_) F1_rfc = f1_score(y1_test,grid.fit(X1_train,y1_train).predict(X1_test), average='micro') {'criterion': 'entropy', 'max_depth': 20, 'max_features': 6, 'max_leaf_nodes': None} 0.9859387923904053 So, with above parameters for random forest model, we’ve reached 98.6% accuracy. curve=learning_curve(RandomForestClassifier(n_estimators=50, criterion='entropy', max_features=6, max_depth=20, random_state=51,max_leaf_nodes=None), X1_train,y1_train, cv=5) size=curve[0] train_score=[curve[1][i].mean() for i in range (0,5)] test_score=[curve[2][i].mean() for i in range (0,5)] fig=plt.figure(figsize=(6,4)) plt.plot(size,train_score) plt.plot(size,test_score) Model is overfitting as train accuracy is 1 ,but test accuracy is much less. I’ve already tried changing RFC parameters to tackle overfitting. But, still it is not reduced.To reduce variance, we can Increase number of samples. (It is clear from above graph that incresing number of samples will improve model) Reduce number of features Feature Reduction feature_import = pd.DataFrame([rfc.feature_importances_], columns=X1.columns) print(feature_import) buying maint doors capacity lug_boot safety 0 0.154819 0.151853 0.059702 0.251283 0.094677 0.287667 From feature importances, it is clear that ‘doors’ feature is least impor","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:4:5","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":["Machine learning"],"content":"Model Comparison models=['rbf SVC','Logistic Regression','Decision Tree','Naive Bayes','Random Forest'] f1 = np.array([f1_SVC_rbf, f1_LR, F1_dtree,f1_gnb,F1_rfc]) y_pos = np.arange(len(models)) plt.barh(y_pos, f1) plt.yticks(y_pos, ('rbf SVC','Logistic Regression','Decision Tree','Naive Bayes','Random Forest')) plt.show() score = pd.DataFrame([f1],columns=models) score .dataframe tbody tr th:only-of-type { vertical-align: middle; } rbf SVC Logistic Regression Decision Tree Naive Bayes Random Forest 0.974 0.81 0.96 0.76 0.986 Conclusion SVM rbf Classifier and Random Forest are roughly equally suitable models for this classification context, however, be aware that Random Forest tends to show overfitting, and accuracy won’t get better with trees growing or features reduction. We are able to achieve 98.6% weighted accuracy with this model. – END – ","date":"2019-10-06","objectID":"/2019-10-06-car-evaluation-using-machine-learning/:5:0","tags":["classification","machine learning"],"title":"Beginner's guide for predictive models","uri":"/2019-10-06-car-evaluation-using-machine-learning/"},{"categories":null,"content":"Hey there ![joy](/images/joy division.png) Here is Xue Ni. I go by Nina. I suck at writing bios. ","date":"2019-08-27","objectID":"/2019-08-27-hi-there/:1:0","tags":null,"title":"Hi there","uri":"/2019-08-27-hi-there/"},{"categories":null,"content":"SQL ","date":"0001-01-01","objectID":"/leetcode/:1:0","tags":null,"title":"","uri":"/leetcode/"},{"categories":null,"content":"LeetCode selected problems Only high-quality problems are selected. Pathological problems and entry-level syntax problems are not included. And ‘solution’ column will be updated once I start to work on ‘em! # Problems Solution Level Concept 262 Trips and Users 🤭 Hard Three-way join; filtering 185 Department Top Three Salaries 🤭 Hard Non-equijoin; aggregation; window functionsample 579 Cumulative Salary of Employee 🤭 Hard Self-join; left join; aggregation 601 Human Traffic of Stadium 🤭 Hard Self-join; de-duplication; window 615 Average Salary 🤭 Hard Case; aggregation, join 618 Students Report By Geography 🤭 Hard Full join, pivoting ","date":"0001-01-01","objectID":"/leetcode/:1:1","tags":null,"title":"","uri":"/leetcode/"}]